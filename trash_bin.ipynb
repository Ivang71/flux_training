{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01f8b498-5267-4879-959d-3e985006a789",
   "metadata": {},
   "outputs": [],
   "source": [
    "# very powerful, but discarded because requires at least 40gb cards to run and may be harder to tweak than diffusers\n",
    "!git clone https://github.com/xdit-project/xDiT.git\n",
    "%cd xDiT\n",
    "%pip install flask\n",
    "%pip install -e \".[diffusers,flash-attn]\"\n",
    "!python3 ./http-service/launch_host.py --config ./http-service/config.json\n",
    "%cd /workspace/dtback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "290d259c-2e16-4484-8d56-1816baeed3cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ComfyUI setup\n",
    "%cd  /workspace/dtback\n",
    "!git clone https://github.com/comfyanonymous/ComfyUI.git\n",
    "%cd /workspace/dtback/ComfyUI\n",
    "# !pip install -r requirements.txt\n",
    "%cd /workspace/dtback/ComfyUI/custom_nodes\n",
    "!git clone https://github.com/ltdrdata/ComfyUI-Manager comfyui-manager\n",
    "!git clone https://github.com/Suzie1/ComfyUI_Comfyroll_CustomNodes.git\n",
    "\n",
    "!huggingface-cli download black-forest-labs/FLUX.1-Fill-dev flux1-fill-dev.safetensors --local-dir /workspace/dtback/ComfyUI/models/unet\n",
    "\n",
    "!huggingface-cli download black-forest-labs/FLUX.1-dev ae.safetensors --local-dir /workspace/dtback/ComfyUI/models/vae\n",
    "!huggingface-cli download comfyanonymous/flux_text_encoders clip_l.safetensors --local-dir /workspace/dtback/ComfyUI/models/clip\n",
    "!huggingface-cli download comfyanonymous/flux_text_encoders t5xxl_fp16.safetensors --local-dir /workspace/dtback/ComfyUI/models/clip\n",
    "\n",
    "!huggingface-cli download kudzueye/Boreal boreal-flux-dev-lora-v04_1000_steps.safetensors --local-dir /workspace/dtback/ComfyUI/models/loras\n",
    "\n",
    "!curl -sSL https://ngrok-agent.s3.amazonaws.com/ngrok.asc | sudo tee /etc/apt/trusted.gpg.d/ngrok.asc >/dev/null && echo \"deb https://ngrok-agent.s3.amazonaws.com buster main\" \\\n",
    "\t| sudo tee /etc/apt/sources.list.d/ngrok.list && sudo apt update && sudo apt install ngrok\n",
    "import base64\n",
    "t = 'Mk9WSkJ6c2p4QzJzcVZLdEFJNjBWdndhWmhEXzd1UUMzVm8xZ3dMVmllS0JOcHdvNg=='\n",
    "k = base64.b64decode(t.encode()).decode()\n",
    "!ngrok config add-authtoken {k}\n",
    "# ngrok http 8000\n",
    "\n",
    "%cd /workspace/dtback\n",
    "!python3 ComfyUI/main.py --port 8000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60da06c9-49d6-40cd-bd87-aa8d1cc11e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# img2img amine\n",
    "%cd ~\n",
    "import time\n",
    "start = time.time()\n",
    "output = flux.img_to_img(\n",
    "    prompt=\"\"\"\n",
    "    \n",
    "    anime picture of two boys standing in a room near a table with pizza\n",
    "\n",
    "    \"\"\",\n",
    "    image=\"img2.jpg\",\n",
    "    strength=0.63,\n",
    "    sample_steps=20,\n",
    "    width=1024,\n",
    "    height=768,\n",
    "    seed=1000,\n",
    "    cfg_scale=5,\n",
    "    style_strength=20,\n",
    "    sample_method=\"euler\",\n",
    ")\n",
    "delta = round(time.time() - start)\n",
    "output[0].convert('RGB').save(\"output_img.jpg\")\n",
    "from IPython.display import clear_output\n",
    "clear_output()\n",
    "print(f'took {delta} seconds')\n",
    "output[0].show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "341a2fd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#install decord\n",
    "!sudo apt update\n",
    "!sudo apt install libnvidia-decode-550-server -y\n",
    "!sudo add-apt-repository ppa:jonathonf/ffmpeg-4 -y\n",
    "!sudo apt-get update\n",
    "!sudo apt-get install -y build-essential python3-dev python3-setuptools make cmake\n",
    "!sudo apt-get install -y ffmpeg libavcodec-dev libavfilter-dev libavformat-dev libavutil-dev\n",
    "!git clone --recursive https://github.com/dmlc/decord\n",
    "%cd decord\n",
    "!mkdir build\n",
    "%cd build\n",
    "!cmake .. -DUSE_CUDA=ON -DCMAKE_BUILD_TYPE=Release\n",
    "!make\n",
    "%cd ../python\n",
    "!python3 setup.py install --user\n",
    "%cd ../.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8fd8063",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess, zipfile, json\n",
    "import pandas as pd\n",
    "\n",
    "def download_tmdb_dataset():\n",
    "    subprocess.run([\n",
    "        \"kaggle\", \"datasets\", \"download\", \"-d\", \"asaniczka/tmdb-movies-dataset-2023-930k-movies\"\n",
    "    ], check=True)\n",
    "    zip_filename = \"tmdb-movies-dataset-2023-930k-movies.zip\"\n",
    "    with zipfile.ZipFile(zip_filename, \"r\") as z:\n",
    "        z.extractall(\".\")\n",
    "    \n",
    "    print(\"Dataset downloaded and extracted.\")\n",
    "\n",
    "download_tmdb_dataset()\n",
    "\n",
    "df = pd.read_csv(\"TMDB_movie_dataset_v11.csv\")\n",
    "df_sorted = df.sort_values(by=\"vote_count\", ascending=False)\n",
    "titles = df_sorted.head(3000)['title'].to_numpy().tolist()\n",
    "\n",
    "with open(\"./dataset_creation/movies.json\", \"w\") as f:\n",
    "    json.dump(titles, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff39e15b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot a cluster\n",
    "import random, math\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "def display_cluster_images(cluster, face_metadata, images_per_row=5, max_images=60):\n",
    "    if len(cluster) > max_images:\n",
    "        cluster = random.sample(cluster, max_images)\n",
    "\n",
    "    num_images = len(cluster)\n",
    "    num_rows = math.ceil(num_images / images_per_row)\n",
    "    fig, axes = plt.subplots(num_rows, images_per_row, figsize=(15, 3 * num_rows))\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    for ax, idx in zip(axes, cluster):\n",
    "        img_path = face_metadata[idx][\"image_path\"]\n",
    "        img = mpimg.imread(img_path)\n",
    "        ax.imshow(img)\n",
    "        ax.axis(\"off\")\n",
    "\n",
    "        # Draw face bbox if available\n",
    "        if \"face_bbox\" in face_metadata[idx] and face_metadata[idx][\"face_bbox\"]:\n",
    "            fb = face_metadata[idx][\"face_bbox\"][0]\n",
    "            x1, y1, x2, y2 = map(int, fb)\n",
    "            rect_face = patches.Rectangle(\n",
    "                (x1, y1), x2 - x1, y2 - y1,\n",
    "                linewidth=2, edgecolor=\"red\", facecolor=\"none\"\n",
    "            )\n",
    "            ax.add_patch(rect_face)\n",
    "\n",
    "        # Draw body bbox if available\n",
    "        if \"body_bbox\" in face_metadata[idx] and face_metadata[idx][\"body_bbox\"]:\n",
    "            bb = face_metadata[idx][\"body_bbox\"]\n",
    "            x1, y1, x2, y2 = map(int, bb)\n",
    "            rect_body = patches.Rectangle(\n",
    "                (x1, y1), x2 - x1, y2 - y1,\n",
    "                linewidth=2, edgecolor=\"blue\", facecolor=\"none\"\n",
    "            )\n",
    "            ax.add_patch(rect_body)\n",
    "\n",
    "    for ax in axes[num_images:]:\n",
    "        ax.axis(\"off\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "display_cluster_images(clusters[6], metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b81c1928",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training\n",
    "\n",
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "\"\"\"\n",
    "Example: Training FLUX.1-dev for text-to-image generation with identity preservation.\n",
    "All base FLUX weights are frozen; only new face/body modules are trainable.\n",
    "\n",
    "After training, we also show how to do a \"normal\" text-to-image generation with FluxPipeline.\n",
    "\"\"\"\n",
    "\n",
    "import os, requests, cv2, torch\n",
    "import insightface\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Instead of DiffusionPipeline, we directly import FluxPipeline\n",
    "from diffusers import FluxPipeline\n",
    "\n",
    "from transformers import AutoProcessor, AutoModelForZeroShotObjectDetection\n",
    "from transformers import AutoModel, AutoFeatureExtractor\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "# =====================================================================\n",
    "# 1) LOAD & FREEZE THE FLUX.1-DEV PIPELINE\n",
    "# =====================================================================\n",
    "print(\"Loading and freezing FLUX.1-dev ...\")\n",
    "pipe = FluxPipeline.from_pretrained(\n",
    "    \"black-forest-labs/FLUX.1-dev\",\n",
    "    device_map=\"balanced\",          # as per your example usage\n",
    "    torch_dtype=torch.bfloat16 if device.type == 'cuda' else torch.float32\n",
    ")\n",
    "\n",
    "# Freeze submodules if they exist\n",
    "if hasattr(pipe, \"transformer\") and pipe.transformer is not None:\n",
    "    for param in pipe.transformer.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "if hasattr(pipe, \"text_encoder\") and pipe.text_encoder is not None:\n",
    "    for param in pipe.text_encoder.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "if hasattr(pipe, \"text_encoder_2\") and pipe.text_encoder_2 is not None:\n",
    "    for param in pipe.text_encoder_2.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "if hasattr(pipe, \"vae\") and pipe.vae is not None:\n",
    "    for param in pipe.vae.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "\n",
    "# =====================================================================\n",
    "# 2) GROUNDING DINO FOR BODY BOUNDING BOX\n",
    "# =====================================================================\n",
    "print(\"Loading GroundingDINO ...\")\n",
    "g_model_id = \"IDEA-Research/grounding-dino-tiny\"\n",
    "g_processor = AutoProcessor.from_pretrained(g_model_id)\n",
    "g_model = AutoModelForZeroShotObjectDetection.from_pretrained(g_model_id).to(device)\n",
    "\n",
    "def detect_body_bbox_pil(pil_image, query_text=\"a person.\", threshold=0.4, text_threshold=0.3):\n",
    "    \"\"\"\n",
    "    Given a PIL image, uses GroundingDINO to detect bounding boxes\n",
    "    for the given query (default: \"a person.\").\n",
    "    Returns the bounding box (xmin, ymin, xmax, ymax) of the largest box found,\n",
    "    or None if no detection.\n",
    "    \"\"\"\n",
    "    inputs = g_processor(images=pil_image, text=query_text, return_tensors=\"pt\").to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = g_model(**inputs)\n",
    "\n",
    "    results = g_processor.post_process_grounded_object_detection(\n",
    "        outputs,\n",
    "        inputs.input_ids,\n",
    "        box_threshold=threshold,\n",
    "        text_threshold=text_threshold,\n",
    "        target_sizes=[pil_image.size[::-1]]  # (height, width)\n",
    "    )\n",
    "    if not results or len(results[0][\"boxes\"]) == 0:\n",
    "        return None\n",
    "\n",
    "    boxes = results[0][\"boxes\"]  # (N, 4)\n",
    "    areas = (boxes[:, 2] - boxes[:, 0]) * (boxes[:, 3] - boxes[:, 1])\n",
    "    max_idx = torch.argmax(areas)\n",
    "    box = boxes[max_idx].tolist()  # [xmin, ymin, xmax, ymax]\n",
    "    return tuple(map(int, box))\n",
    "\n",
    "\n",
    "# =====================================================================\n",
    "# 3) INTERNVIT FOR BODY EMBEDDINGS\n",
    "# =====================================================================\n",
    "print(\"Loading InternViT-300M-448px ...\")\n",
    "internvit_model = AutoModel.from_pretrained(\"OpenGVLab/InternViT-300M-448px-V2_5\", trust_remote_code=True).to(device)\n",
    "internvit_extractor = AutoFeatureExtractor.from_pretrained(\"OpenGVLab/InternViT-300M-448px-V2_5\", trust_remote_code=True)\n",
    "internvit_model.eval()\n",
    "for p in internvit_model.parameters():\n",
    "    p.requires_grad = False\n",
    "\n",
    "def extract_body_embedding_pil(pil_image, bbox=None):\n",
    "    \"\"\"\n",
    "    Crops the bounding box region from a PIL image and extracts a body embedding.\n",
    "    If bbox is None, use the entire image.\n",
    "    \"\"\"\n",
    "    if bbox is not None:\n",
    "        xmin, ymin, xmax, ymax = bbox\n",
    "        pil_crop = pil_image.crop((xmin, ymin, xmax, ymax))\n",
    "    else:\n",
    "        pil_crop = pil_image\n",
    "\n",
    "    inputs = internvit_extractor(images=pil_crop, return_tensors=\"pt\").to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = internvit_model(**inputs)\n",
    "    if hasattr(outputs, \"pooler_output\"):\n",
    "        emb = outputs.pooler_output  # (1, hidden_dim)\n",
    "    else:\n",
    "        emb = outputs.last_hidden_state[:, 0, :]\n",
    "    return emb  # (1, hidden_dim)\n",
    "\n",
    "\n",
    "# =====================================================================\n",
    "# 4) INSIGHTFACE FOR FACE EMBEDDINGS\n",
    "# =====================================================================\n",
    "print(\"Initializing InsightFace ...\")\n",
    "face_analysis = insightface.app.FaceAnalysis()\n",
    "ctx_id = 0 if device.type == 'cuda' else -1\n",
    "face_analysis.prepare(ctx_id=ctx_id, det_size=(640, 640))\n",
    "\n",
    "def extract_face_embedding_pil(pil_image):\n",
    "    \"\"\"\n",
    "    Convert PIL to BGR numpy, pass to insightface, return 512-dim face embedding.\n",
    "    If no face is found, returns zeros.\n",
    "    \"\"\"\n",
    "    np_img = np.array(pil_image)[:, :, ::-1]  # RGB -> BGR\n",
    "    faces = face_analysis.get(np_img)\n",
    "    if len(faces) == 0:\n",
    "        return torch.zeros((1, 512), device=device)\n",
    "    face = faces[0]\n",
    "    emb = face.normed_embedding  # (512,)\n",
    "    return torch.tensor(emb, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "\n",
    "\n",
    "# =====================================================================\n",
    "# 5) NEW MODULES: PERCEIVER + CROSS-ATTENTION\n",
    "# =====================================================================\n",
    "class CrossAttentionBlock(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads):\n",
    "        super().__init__()\n",
    "        self.attn = nn.MultiheadAttention(embed_dim, num_heads, batch_first=True)\n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(embed_dim, embed_dim * 4),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(embed_dim * 4, embed_dim)\n",
    "        )\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "\n",
    "    def forward(self, x, context):\n",
    "        attn_output, _ = self.attn(x, context, context)\n",
    "        x = self.norm1(x + attn_output)\n",
    "        mlp_output = self.mlp(x)\n",
    "        x = self.norm2(x + mlp_output)\n",
    "        return x\n",
    "\n",
    "class PerceiverResampler(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim, num_tokens=8):\n",
    "        super().__init__()\n",
    "        self.num_tokens = num_tokens\n",
    "        self.proj = nn.Linear(in_dim, out_dim * num_tokens)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B = x.shape[0]\n",
    "        out = self.proj(x).view(B, self.num_tokens, -1)\n",
    "        return out\n",
    "\n",
    "\n",
    "# =====================================================================\n",
    "# 6) WRAPPER: Freeze FLUX, Insert New Modules\n",
    "# =====================================================================\n",
    "class FluxFrozenWrapper(nn.Module):\n",
    "    \"\"\"\n",
    "    We assume everything in 'pipe' is frozen. \n",
    "    We add face/body resamplers + cross-attention blocks as trainable.\n",
    "    \"\"\"\n",
    "    def __init__(self, pipe, embed_dim=768, num_heads=8, face_dim=512, body_dim=768):\n",
    "        super().__init__()\n",
    "        self.pipe = pipe  # The frozen FluxPipeline\n",
    "\n",
    "        # Trainable modules\n",
    "        self.face_resampler = PerceiverResampler(face_dim, embed_dim, num_tokens=8)\n",
    "        self.body_resampler = PerceiverResampler(body_dim, embed_dim, num_tokens=8)\n",
    "        self.face_cross_attn = CrossAttentionBlock(embed_dim, num_heads)\n",
    "        self.body_cross_attn = CrossAttentionBlock(embed_dim, num_heads)\n",
    "\n",
    "    def forward_unet(self, latents, t, text_embeddings, face_tokens, body_tokens):\n",
    "        \"\"\"\n",
    "        Example approach to integrate with the frozen pipeline's UNet-like model.\n",
    "        We'll assume self.pipe.unet(...) is valid in FLUX (the config suggests\n",
    "        'transformer' is the underlying 2D model, but we'll keep the naming for demonstration).\n",
    "        \"\"\"\n",
    "        unet_out = self.pipe.unet(\n",
    "            latents, t, encoder_hidden_states=text_embeddings\n",
    "        ).sample  # shape: (B, 4, H, W)\n",
    "\n",
    "        B, C, H, W = unet_out.shape\n",
    "        unet_out_reshaped = unet_out.view(B, C * H * W).unsqueeze(1).contiguous()\n",
    "\n",
    "        face_attended = self.face_cross_attn(unet_out_reshaped, face_tokens)\n",
    "        body_attended = self.body_cross_attn(face_attended, body_tokens)\n",
    "\n",
    "        body_attended = body_attended.view(B, C, H, W)\n",
    "        return body_attended\n",
    "\n",
    "    def forward(self, latents, t, text_embeddings, face_emb, body_emb):\n",
    "        face_tokens = self.face_resampler(face_emb)\n",
    "        body_tokens = self.body_resampler(body_emb)\n",
    "        out_latents = self.forward_unet(latents, t, text_embeddings, face_tokens, body_tokens)\n",
    "        return out_latents\n",
    "\n",
    "\n",
    "# =====================================================================\n",
    "# 7) EXAMPLE DATASET\n",
    "# =====================================================================\n",
    "class ExampleCharacterDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Minimal example: each item has:\n",
    "      - image_path: path to the reference image\n",
    "      - prompt: text prompt\n",
    "    We'll generate random latents and timesteps for demonstration.\n",
    "    \"\"\"\n",
    "    def __init__(self, image_paths, prompts):\n",
    "        self.image_paths = image_paths\n",
    "        self.prompts = prompts\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_paths[idx]\n",
    "        pil_image = Image.open(img_path).convert(\"RGB\")\n",
    "        prompt = self.prompts[idx]\n",
    "\n",
    "        latents = torch.randn((4, 64, 64))  # mock latents\n",
    "        t = torch.randint(0, 1000, (1,))\n",
    "        return {\n",
    "            \"pil_image\": pil_image,\n",
    "            \"prompt\": prompt,\n",
    "            \"latents\": latents,\n",
    "            \"timestep\": t\n",
    "        }\n",
    "\n",
    "\n",
    "# =====================================================================\n",
    "# 8) TRAINING FUNCTION\n",
    "# =====================================================================\n",
    "def diffusion_loss_fn(pred_latents, target_latents):\n",
    "    \"\"\"\n",
    "    Typical MSE on latents (mock).\n",
    "    In real usage, you'd have a target latents or predicted noise approach.\n",
    "    \"\"\"\n",
    "    return F.mse_loss(pred_latents, target_latents)\n",
    "\n",
    "def train_identity_preservation(\n",
    "    flux_wrapper,\n",
    "    pipe,\n",
    "    dataloader,\n",
    "    epochs=1,\n",
    "    lr=1e-4,\n",
    "    lambda_face=1.0,\n",
    "    lambda_body=1.0\n",
    "):\n",
    "    \"\"\"\n",
    "    - flux_wrapper: The model with new face/body modules.\n",
    "    - pipe: The original FluxPipeline (frozen).\n",
    "    - dataloader: yields reference images, prompts, latents, timesteps\n",
    "    \"\"\"\n",
    "    trainable_params = [p for p in flux_wrapper.parameters() if p.requires_grad]\n",
    "    optimizer = optim.Adam(trainable_params, lr=lr)\n",
    "\n",
    "    flux_wrapper.train()\n",
    "    flux_wrapper.to(device)\n",
    "\n",
    "    # Check that the submodules are frozen\n",
    "    for submodel in [pipe.transformer, pipe.text_encoder, pipe.text_encoder_2, pipe.vae]:\n",
    "        if submodel is not None:\n",
    "            for name, p in submodel.named_parameters():\n",
    "                assert not p.requires_grad, f\"Parameter {name} should be frozen!\"\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        for step_idx, batch in enumerate(dataloader):\n",
    "            pil_image = batch[\"pil_image\"]\n",
    "            prompt = batch[\"prompt\"]\n",
    "            latents = batch[\"latents\"].to(device, dtype=torch.float32)\n",
    "            t = batch[\"timestep\"].to(device)\n",
    "\n",
    "            # -------------------------\n",
    "            # 1) TEXT ENCODING\n",
    "            # -------------------------\n",
    "            if hasattr(pipe, \"tokenizer\") and hasattr(pipe, \"text_encoder\"):\n",
    "                text_in = pipe.tokenizer(prompt, padding=\"max_length\", max_length=77, return_tensors=\"pt\").to(device)\n",
    "                with torch.no_grad():\n",
    "                    text_out = pipe.text_encoder(**text_in)\n",
    "                if hasattr(text_out, \"last_hidden_state\"):\n",
    "                    text_embeddings = text_out.last_hidden_state\n",
    "                else:\n",
    "                    text_embeddings = text_out\n",
    "            else:\n",
    "                # fallback if no built-in text encoder\n",
    "                b_size = latents.shape[0]\n",
    "                text_embeddings = torch.randn((b_size, 77, 768), device=device)\n",
    "\n",
    "            # -------------------------\n",
    "            # 2) FACE & BODY EMBEDDINGS\n",
    "            # -------------------------\n",
    "            face_emb_list = []\n",
    "            body_emb_list = []\n",
    "            for b in range(latents.shape[0]):\n",
    "                img = pil_image[b] if isinstance(pil_image, list) else pil_image\n",
    "                bbox = detect_body_bbox_pil(img, query_text=\"a person.\")\n",
    "                body_emb = extract_body_embedding_pil(img, bbox)\n",
    "                body_emb_list.append(body_emb)\n",
    "\n",
    "                face_emb = extract_face_embedding_pil(img)\n",
    "                face_emb_list.append(face_emb)\n",
    "\n",
    "            face_emb_tensor = torch.cat(face_emb_list, dim=0).to(device)\n",
    "            body_emb_tensor = torch.cat(body_emb_list, dim=0).to(device)\n",
    "\n",
    "            # -------------------------\n",
    "            # 3) FORWARD PASS\n",
    "            # -------------------------\n",
    "            pred_latents = flux_wrapper(latents, t, text_embeddings, face_emb_tensor, body_emb_tensor)\n",
    "\n",
    "            # Diffusion MSE (mock)\n",
    "            diff_loss = diffusion_loss_fn(pred_latents, latents)\n",
    "\n",
    "            # -------------------------\n",
    "            # 4) DECODE & IDENTITY LOSS\n",
    "            # -------------------------\n",
    "            with torch.no_grad():\n",
    "                if pred_latents.dtype != pipe.vae.dtype:\n",
    "                    pred_latents = pred_latents.to(pipe.vae.dtype)\n",
    "                decoded = pipe.vae.decode(pred_latents).sample  # (B, 3, H, W)\n",
    "\n",
    "            face_loss_val = 0.0\n",
    "            body_loss_val = 0.0\n",
    "            b_sz = decoded.shape[0]\n",
    "\n",
    "            for b in range(b_sz):\n",
    "                # (3, H, W) -> PIL\n",
    "                img_np = decoded[b].detach().cpu().float().numpy()\n",
    "                img_min, img_max = img_np.min(), img_np.max()\n",
    "                img_np = (img_np - img_min) / (img_max - img_min + 1e-8)\n",
    "                img_np = (img_np * 255).astype(np.uint8)\n",
    "                img_np = np.transpose(img_np, (1, 2, 0))\n",
    "                pil_decoded = Image.fromarray(img_np)\n",
    "\n",
    "                gen_face_emb = extract_face_embedding_pil(pil_decoded)\n",
    "                f_loss = F.mse_loss(gen_face_emb, face_emb_tensor[b:b+1])\n",
    "                face_loss_val += f_loss.item()\n",
    "\n",
    "                dec_bbox = detect_body_bbox_pil(pil_decoded, query_text=\"a person.\")\n",
    "                gen_body_emb = extract_body_embedding_pil(pil_decoded, dec_bbox)\n",
    "                b_loss = F.mse_loss(gen_body_emb, body_emb_tensor[b:b+1])\n",
    "                body_loss_val += b_loss.item()\n",
    "\n",
    "            face_loss_val /= b_sz\n",
    "            body_loss_val /= b_sz\n",
    "\n",
    "            face_loss = torch.tensor(face_loss_val, device=device, requires_grad=True)\n",
    "            body_loss = torch.tensor(body_loss_val, device=device, requires_grad=True)\n",
    "\n",
    "            total_loss = diff_loss + lambda_face * face_loss + lambda_body * body_loss\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            total_loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if step_idx % 5 == 0:\n",
    "                print(f\"Epoch {epoch} | Step {step_idx} | \"\n",
    "                      f\"Diff={diff_loss.item():.4f} | Face={face_loss_val:.4f} | Body={body_loss_val:.4f} | \"\n",
    "                      f\"Total={total_loss.item():.4f}\")\n",
    "\n",
    "\n",
    "# =====================================================================\n",
    "# 9) MAIN / DEMO\n",
    "# =====================================================================\n",
    "def main():\n",
    "    # Example dataset\n",
    "    image_paths = [\"./example1.jpg\", \"./example2.jpg\"]\n",
    "    prompts = [\"Character in a futuristic city\", \"Character on the beach\"]\n",
    "    dataset = ExampleCharacterDataset(image_paths, prompts)\n",
    "    dataloader = DataLoader(dataset, batch_size=1, shuffle=True)\n",
    "\n",
    "    # Wrap the pipeline with new modules\n",
    "    flux_wrapper = FluxFrozenWrapper(\n",
    "        pipe,\n",
    "        embed_dim=768,\n",
    "        num_heads=8,\n",
    "        face_dim=512,\n",
    "        body_dim=768\n",
    "    ).to(device)\n",
    "\n",
    "    # Confirm submodules are frozen\n",
    "    for submodel in [pipe.transformer, pipe.text_encoder, pipe.text_encoder_2, pipe.vae]:\n",
    "        if submodel is not None:\n",
    "            for n, p in submodel.named_parameters():\n",
    "                assert not p.requires_grad, f\"Param {n} should be frozen!\"\n",
    "\n",
    "    # The only trainable parameters are in flux_wrapper\n",
    "    for n, p in flux_wrapper.named_parameters():\n",
    "        print(f\"{n} | requires_grad={p.requires_grad}\")\n",
    "\n",
    "    # 1) Run a quick training loop\n",
    "    train_identity_preservation(\n",
    "        flux_wrapper,\n",
    "        pipe,\n",
    "        dataloader,\n",
    "        epochs=1,\n",
    "        lr=1e-4,\n",
    "        lambda_face=1.0,\n",
    "        lambda_body=1.0\n",
    "    )\n",
    "\n",
    "    # 2) Demonstrate normal usage of the pipeline for generation\n",
    "    #    (You can do this after training, or skip if not needed)\n",
    "    print(\"\\nGenerating an image with the pipeline after training:\")\n",
    "    img = pipe(\n",
    "        prompt=\"woman\",\n",
    "        guidance_scale=2,\n",
    "        height=1024,\n",
    "        width=1024,\n",
    "        num_inference_steps=40,\n",
    "        generator=torch.Generator(\"cuda\").manual_seed(10)\n",
    "    ).images[0]\n",
    "\n",
    "    img.show()\n",
    "    img.save('after_training_generation.webp')\n",
    "    print(\"Saved 'after_training_generation.webp'\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a93cae4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd dataset_creation\n",
    "\n",
    "import os, asyncio, base64, shutil, random, string, logging, sys, requests, glob, cv2, re\n",
    "import insightface, torch, pickle, time, bencodepy, hashlib,  json, subprocess, multiprocessing\n",
    "import os\n",
    "import asyncio\n",
    "import subprocess\n",
    "import json\n",
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "def get_video_duration(v):\n",
    "    cmd = [\"ffprobe\", \"-v\", \"quiet\", \"-print_format\", \"json\", \"-show_format\", \"-show_streams\", v]\n",
    "    out = subprocess.check_output(cmd, stderr=subprocess.STDOUT)\n",
    "    return float(json.loads(out)['format']['duration'])\n",
    "\n",
    "async def extract_segment(name, v, s, d, idx):\n",
    "    od = f\"./data/raw_frames/{name}/segment_{idx}\"\n",
    "    os.makedirs(od, exist_ok=True)\n",
    "    cmd = [\"ffmpeg\", \"-hide_banner\", \"-hwaccel\", \"cuda\", \"-c:v\", \"h264_cuvid\", \"-ss\", str(s), \"-t\", str(d),\n",
    "           \"-i\", v, \"-vf\", \"fps=0.5\", \"-q:v\", \"2\", \"-vsync\", \"0\", \"-threads\", \"0\", f\"{od}/frame_%04d.jpg\"]\n",
    "    proc = await asyncio.create_subprocess_exec(*cmd)\n",
    "    await proc.wait()\n",
    "\n",
    "def merge_frames(name, parts):\n",
    "    bd = f\"./data/raw_frames/{name}\"\n",
    "    cnt = 1\n",
    "    for i in range(1, parts + 1):\n",
    "        seg = os.path.join(bd, f\"segment_{i}\")\n",
    "        for f in sorted(glob.glob(os.path.join(seg, '*.jpg'))):\n",
    "            shutil.move(f, os.path.join(bd, f\"frame_{cnt:04d}.jpg\"))\n",
    "            cnt += 1\n",
    "        os.rmdir(seg)\n",
    "\n",
    "async def extract_frames(name, v, parts=8):\n",
    "    bd = f\"./data/raw_frames/{name}\"\n",
    "    os.makedirs(bd, exist_ok=True)\n",
    "    total = get_video_duration(v)\n",
    "    d = total / parts\n",
    "    tasks = [asyncio.create_task(extract_segment(name, v, i * d, d, i + 1)) for i in range(parts)]\n",
    "    await asyncio.gather(*tasks)\n",
    "    merge_frames(name, parts)\n",
    "\n",
    "asyncio.run(extract_frames(\"DBqi0K75\", \"./data/vids/DBqi0K75.mp4\", parts=8))\n",
    "\n",
    "%cd .."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
