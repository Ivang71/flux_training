{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "HOME = os.getcwd()\n",
    "print(HOME)\n",
    "\n",
    "!export MAKEFLAGS=\"-j$(nproc)\"\n",
    "!pip install diffusers transformers accelerate xformers huggingface_hub[hf_transfer] hf_transfer \\\n",
    "    pillow insightface opencv-python apex gradio onnxruntime-gpu timm pickleshare \\\n",
    "    SentencePiece ftfy einops facexlib fire onnx onnxruntime-gpu\n",
    "!pip show basicsr || pip install git+https://github.com/XPixelGroup/BasicSR\n",
    "!pip install flash-attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "from huggingface_hub import login\n",
    "import base64\n",
    "k = base64.b64decode('aGZfaHZqck9VTXFvTXF3dW9HR3JoTlZKSWlsZUtFTlNQbXRjTw==').decode()\n",
    "login(token=k, add_to_git_credential=False)\n",
    "%env HUGGINGFACEHUB_API_TOKEN={k}\n",
    "%env HF_TOKEN={k}\n",
    "%env HF_HUB_ENABLE_HF_TRANSFER=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !sudo -v ; curl https://rclone.org/install.sh | sudo bash\n",
    "# !sudo apt-get update\n",
    "# !apt-get update && apt-get install -y fuse3 -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, subprocess, os, asyncio\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "\n",
    "def upload_to_drive():\n",
    "    DATASET_DIR = \"dataset_creation/data/dataset\"\n",
    "    REMOTE_BASE = \"drive:dataset\"\n",
    "    bundles = [d for d in os.listdir(DATASET_DIR)\n",
    "               if os.path.isdir(os.path.join(DATASET_DIR, d)) and d.isdigit()]\n",
    "    for bundle in bundles:\n",
    "        archive_name = f\"{bundle}.tar\"  # uncompressed archive\n",
    "        subprocess.run([\"tar\", \"-cf\", archive_name, \"-C\", DATASET_DIR, bundle], check=True)\n",
    "        subprocess.run([\n",
    "            \"rclone\", \"copy\", archive_name, REMOTE_BASE,\n",
    "            \"--transfers=32\", \"--checkers=32\", \"--fast-list\", \"--progress\"\n",
    "        ], check=True)\n",
    "        os.remove(archive_name)\n",
    "        print(f\"Processed and uploaded bundle {bundle}\")\n",
    "\n",
    "\n",
    "async def main():\n",
    "    movies_path = \"./dataset_creation/movies.json\"\n",
    "    processed_path = \"./dataset_creation/processed.json\"\n",
    "    \n",
    "    with open(movies_path, \"r\") as f:\n",
    "        movies = json.load(f)\n",
    "    if os.path.exists(processed_path):\n",
    "        with open(processed_path, \"r\") as f:\n",
    "            processed = json.load(f)\n",
    "    else:\n",
    "        processed = []\n",
    "    \n",
    "    upload_tasks = []\n",
    "    \n",
    "    while movies:\n",
    "        movie = movies.pop(0)\n",
    "        result = subprocess.run([\"python3\", \"add_movie.py\", \"--movie_name\", movie],\n",
    "                                cwd=\"dataset_creation\")\n",
    "        if result.returncode == 0:\n",
    "            print(f\"Successfully processed {movie}\")\n",
    "            processed.append(movie)\n",
    "        else:\n",
    "            print(f\"Processing error ({result.returncode}) for {movie}\")\n",
    "        \n",
    "        with open(movies_path, \"w\") as f:\n",
    "            json.dump(movies, f, indent=4)\n",
    "        with open(processed_path, \"w\") as f:\n",
    "            json.dump(processed, f, indent=4)\n",
    "        \n",
    "        # Launch the upload function in the background asynchronously.\n",
    "        # This runs upload_to_drive() in a separate thread.\n",
    "        task = asyncio.create_task(asyncio.to_thread(upload_to_drive))\n",
    "        upload_tasks.append(task)\n",
    "        \n",
    "        # Optionally yield control so upload can start concurrently.\n",
    "        await asyncio.sleep(0)\n",
    "    \n",
    "    # Wait for all upload tasks to finish before exiting.\n",
    "    await asyncio.gather(*upload_tasks)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    asyncio.run(main())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!huggingface-cli download black-forest-labs/FLUX.1-dev\n",
    "!huggingface-cli download OpenGVLab/InternViT-300M-448px-V2_5\n",
    "!huggingface-cli download IDEA-Research/grounding-dino-tiny\n",
    "# !huggingface-cli download OpenGVLab/InternVL2-26B # for dataset creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "\"\"\"\n",
    "Example: Training FLUX.1-dev for text-to-image generation with identity preservation.\n",
    "All base FLUX weights are frozen; only new face/body modules are trainable.\n",
    "\n",
    "After training, we also show how to do a \"normal\" text-to-image generation with FluxPipeline.\n",
    "\"\"\"\n",
    "\n",
    "import os, requests, cv2, torch\n",
    "import insightface\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Instead of DiffusionPipeline, we directly import FluxPipeline\n",
    "from diffusers import FluxPipeline\n",
    "\n",
    "from transformers import AutoProcessor, AutoModelForZeroShotObjectDetection\n",
    "from transformers import AutoModel, AutoFeatureExtractor\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "# =====================================================================\n",
    "# 1) LOAD & FREEZE THE FLUX.1-DEV PIPELINE\n",
    "# =====================================================================\n",
    "print(\"Loading and freezing FLUX.1-dev ...\")\n",
    "pipe = FluxPipeline.from_pretrained(\n",
    "    \"black-forest-labs/FLUX.1-dev\",\n",
    "    device_map=\"balanced\",          # as per your example usage\n",
    "    torch_dtype=torch.bfloat16 if device.type == 'cuda' else torch.float32\n",
    ")\n",
    "\n",
    "# Freeze submodules if they exist\n",
    "if hasattr(pipe, \"transformer\") and pipe.transformer is not None:\n",
    "    for param in pipe.transformer.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "if hasattr(pipe, \"text_encoder\") and pipe.text_encoder is not None:\n",
    "    for param in pipe.text_encoder.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "if hasattr(pipe, \"text_encoder_2\") and pipe.text_encoder_2 is not None:\n",
    "    for param in pipe.text_encoder_2.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "if hasattr(pipe, \"vae\") and pipe.vae is not None:\n",
    "    for param in pipe.vae.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "\n",
    "# =====================================================================\n",
    "# 2) GROUNDING DINO FOR BODY BOUNDING BOX\n",
    "# =====================================================================\n",
    "print(\"Loading GroundingDINO ...\")\n",
    "g_model_id = \"IDEA-Research/grounding-dino-tiny\"\n",
    "g_processor = AutoProcessor.from_pretrained(g_model_id)\n",
    "g_model = AutoModelForZeroShotObjectDetection.from_pretrained(g_model_id).to(device)\n",
    "\n",
    "def detect_body_bbox_pil(pil_image, query_text=\"a person.\", threshold=0.4, text_threshold=0.3):\n",
    "    \"\"\"\n",
    "    Given a PIL image, uses GroundingDINO to detect bounding boxes\n",
    "    for the given query (default: \"a person.\").\n",
    "    Returns the bounding box (xmin, ymin, xmax, ymax) of the largest box found,\n",
    "    or None if no detection.\n",
    "    \"\"\"\n",
    "    inputs = g_processor(images=pil_image, text=query_text, return_tensors=\"pt\").to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = g_model(**inputs)\n",
    "\n",
    "    results = g_processor.post_process_grounded_object_detection(\n",
    "        outputs,\n",
    "        inputs.input_ids,\n",
    "        box_threshold=threshold,\n",
    "        text_threshold=text_threshold,\n",
    "        target_sizes=[pil_image.size[::-1]]  # (height, width)\n",
    "    )\n",
    "    if not results or len(results[0][\"boxes\"]) == 0:\n",
    "        return None\n",
    "\n",
    "    boxes = results[0][\"boxes\"]  # (N, 4)\n",
    "    areas = (boxes[:, 2] - boxes[:, 0]) * (boxes[:, 3] - boxes[:, 1])\n",
    "    max_idx = torch.argmax(areas)\n",
    "    box = boxes[max_idx].tolist()  # [xmin, ymin, xmax, ymax]\n",
    "    return tuple(map(int, box))\n",
    "\n",
    "\n",
    "# =====================================================================\n",
    "# 3) INTERNVIT FOR BODY EMBEDDINGS\n",
    "# =====================================================================\n",
    "print(\"Loading InternViT-300M-448px ...\")\n",
    "internvit_model = AutoModel.from_pretrained(\"OpenGVLab/InternViT-300M-448px-V2_5\", trust_remote_code=True).to(device)\n",
    "internvit_extractor = AutoFeatureExtractor.from_pretrained(\"OpenGVLab/InternViT-300M-448px-V2_5\", trust_remote_code=True)\n",
    "internvit_model.eval()\n",
    "for p in internvit_model.parameters():\n",
    "    p.requires_grad = False\n",
    "\n",
    "def extract_body_embedding_pil(pil_image, bbox=None):\n",
    "    \"\"\"\n",
    "    Crops the bounding box region from a PIL image and extracts a body embedding.\n",
    "    If bbox is None, use the entire image.\n",
    "    \"\"\"\n",
    "    if bbox is not None:\n",
    "        xmin, ymin, xmax, ymax = bbox\n",
    "        pil_crop = pil_image.crop((xmin, ymin, xmax, ymax))\n",
    "    else:\n",
    "        pil_crop = pil_image\n",
    "\n",
    "    inputs = internvit_extractor(images=pil_crop, return_tensors=\"pt\").to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = internvit_model(**inputs)\n",
    "    if hasattr(outputs, \"pooler_output\"):\n",
    "        emb = outputs.pooler_output  # (1, hidden_dim)\n",
    "    else:\n",
    "        emb = outputs.last_hidden_state[:, 0, :]\n",
    "    return emb  # (1, hidden_dim)\n",
    "\n",
    "\n",
    "# =====================================================================\n",
    "# 4) INSIGHTFACE FOR FACE EMBEDDINGS\n",
    "# =====================================================================\n",
    "print(\"Initializing InsightFace ...\")\n",
    "face_analysis = insightface.app.FaceAnalysis()\n",
    "ctx_id = 0 if device.type == 'cuda' else -1\n",
    "face_analysis.prepare(ctx_id=ctx_id, det_size=(640, 640))\n",
    "\n",
    "def extract_face_embedding_pil(pil_image):\n",
    "    \"\"\"\n",
    "    Convert PIL to BGR numpy, pass to insightface, return 512-dim face embedding.\n",
    "    If no face is found, returns zeros.\n",
    "    \"\"\"\n",
    "    np_img = np.array(pil_image)[:, :, ::-1]  # RGB -> BGR\n",
    "    faces = face_analysis.get(np_img)\n",
    "    if len(faces) == 0:\n",
    "        return torch.zeros((1, 512), device=device)\n",
    "    face = faces[0]\n",
    "    emb = face.normed_embedding  # (512,)\n",
    "    return torch.tensor(emb, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "\n",
    "\n",
    "# =====================================================================\n",
    "# 5) NEW MODULES: PERCEIVER + CROSS-ATTENTION\n",
    "# =====================================================================\n",
    "class CrossAttentionBlock(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads):\n",
    "        super().__init__()\n",
    "        self.attn = nn.MultiheadAttention(embed_dim, num_heads, batch_first=True)\n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(embed_dim, embed_dim * 4),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(embed_dim * 4, embed_dim)\n",
    "        )\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "\n",
    "    def forward(self, x, context):\n",
    "        attn_output, _ = self.attn(x, context, context)\n",
    "        x = self.norm1(x + attn_output)\n",
    "        mlp_output = self.mlp(x)\n",
    "        x = self.norm2(x + mlp_output)\n",
    "        return x\n",
    "\n",
    "class PerceiverResampler(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim, num_tokens=8):\n",
    "        super().__init__()\n",
    "        self.num_tokens = num_tokens\n",
    "        self.proj = nn.Linear(in_dim, out_dim * num_tokens)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B = x.shape[0]\n",
    "        out = self.proj(x).view(B, self.num_tokens, -1)\n",
    "        return out\n",
    "\n",
    "\n",
    "# =====================================================================\n",
    "# 6) WRAPPER: Freeze FLUX, Insert New Modules\n",
    "# =====================================================================\n",
    "class FluxFrozenWrapper(nn.Module):\n",
    "    \"\"\"\n",
    "    We assume everything in 'pipe' is frozen. \n",
    "    We add face/body resamplers + cross-attention blocks as trainable.\n",
    "    \"\"\"\n",
    "    def __init__(self, pipe, embed_dim=768, num_heads=8, face_dim=512, body_dim=768):\n",
    "        super().__init__()\n",
    "        self.pipe = pipe  # The frozen FluxPipeline\n",
    "\n",
    "        # Trainable modules\n",
    "        self.face_resampler = PerceiverResampler(face_dim, embed_dim, num_tokens=8)\n",
    "        self.body_resampler = PerceiverResampler(body_dim, embed_dim, num_tokens=8)\n",
    "        self.face_cross_attn = CrossAttentionBlock(embed_dim, num_heads)\n",
    "        self.body_cross_attn = CrossAttentionBlock(embed_dim, num_heads)\n",
    "\n",
    "    def forward_unet(self, latents, t, text_embeddings, face_tokens, body_tokens):\n",
    "        \"\"\"\n",
    "        Example approach to integrate with the frozen pipeline's UNet-like model.\n",
    "        We'll assume self.pipe.unet(...) is valid in FLUX (the config suggests\n",
    "        'transformer' is the underlying 2D model, but we'll keep the naming for demonstration).\n",
    "        \"\"\"\n",
    "        unet_out = self.pipe.unet(\n",
    "            latents, t, encoder_hidden_states=text_embeddings\n",
    "        ).sample  # shape: (B, 4, H, W)\n",
    "\n",
    "        B, C, H, W = unet_out.shape\n",
    "        unet_out_reshaped = unet_out.view(B, C * H * W).unsqueeze(1).contiguous()\n",
    "\n",
    "        face_attended = self.face_cross_attn(unet_out_reshaped, face_tokens)\n",
    "        body_attended = self.body_cross_attn(face_attended, body_tokens)\n",
    "\n",
    "        body_attended = body_attended.view(B, C, H, W)\n",
    "        return body_attended\n",
    "\n",
    "    def forward(self, latents, t, text_embeddings, face_emb, body_emb):\n",
    "        face_tokens = self.face_resampler(face_emb)\n",
    "        body_tokens = self.body_resampler(body_emb)\n",
    "        out_latents = self.forward_unet(latents, t, text_embeddings, face_tokens, body_tokens)\n",
    "        return out_latents\n",
    "\n",
    "\n",
    "# =====================================================================\n",
    "# 7) EXAMPLE DATASET\n",
    "# =====================================================================\n",
    "class ExampleCharacterDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Minimal example: each item has:\n",
    "      - image_path: path to the reference image\n",
    "      - prompt: text prompt\n",
    "    We'll generate random latents and timesteps for demonstration.\n",
    "    \"\"\"\n",
    "    def __init__(self, image_paths, prompts):\n",
    "        self.image_paths = image_paths\n",
    "        self.prompts = prompts\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_paths[idx]\n",
    "        pil_image = Image.open(img_path).convert(\"RGB\")\n",
    "        prompt = self.prompts[idx]\n",
    "\n",
    "        latents = torch.randn((4, 64, 64))  # mock latents\n",
    "        t = torch.randint(0, 1000, (1,))\n",
    "        return {\n",
    "            \"pil_image\": pil_image,\n",
    "            \"prompt\": prompt,\n",
    "            \"latents\": latents,\n",
    "            \"timestep\": t\n",
    "        }\n",
    "\n",
    "\n",
    "# =====================================================================\n",
    "# 8) TRAINING FUNCTION\n",
    "# =====================================================================\n",
    "def diffusion_loss_fn(pred_latents, target_latents):\n",
    "    \"\"\"\n",
    "    Typical MSE on latents (mock).\n",
    "    In real usage, you'd have a target latents or predicted noise approach.\n",
    "    \"\"\"\n",
    "    return F.mse_loss(pred_latents, target_latents)\n",
    "\n",
    "def train_identity_preservation(\n",
    "    flux_wrapper,\n",
    "    pipe,\n",
    "    dataloader,\n",
    "    epochs=1,\n",
    "    lr=1e-4,\n",
    "    lambda_face=1.0,\n",
    "    lambda_body=1.0\n",
    "):\n",
    "    \"\"\"\n",
    "    - flux_wrapper: The model with new face/body modules.\n",
    "    - pipe: The original FluxPipeline (frozen).\n",
    "    - dataloader: yields reference images, prompts, latents, timesteps\n",
    "    \"\"\"\n",
    "    trainable_params = [p for p in flux_wrapper.parameters() if p.requires_grad]\n",
    "    optimizer = optim.Adam(trainable_params, lr=lr)\n",
    "\n",
    "    flux_wrapper.train()\n",
    "    flux_wrapper.to(device)\n",
    "\n",
    "    # Check that the submodules are frozen\n",
    "    for submodel in [pipe.transformer, pipe.text_encoder, pipe.text_encoder_2, pipe.vae]:\n",
    "        if submodel is not None:\n",
    "            for name, p in submodel.named_parameters():\n",
    "                assert not p.requires_grad, f\"Parameter {name} should be frozen!\"\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        for step_idx, batch in enumerate(dataloader):\n",
    "            pil_image = batch[\"pil_image\"]\n",
    "            prompt = batch[\"prompt\"]\n",
    "            latents = batch[\"latents\"].to(device, dtype=torch.float32)\n",
    "            t = batch[\"timestep\"].to(device)\n",
    "\n",
    "            # -------------------------\n",
    "            # 1) TEXT ENCODING\n",
    "            # -------------------------\n",
    "            if hasattr(pipe, \"tokenizer\") and hasattr(pipe, \"text_encoder\"):\n",
    "                text_in = pipe.tokenizer(prompt, padding=\"max_length\", max_length=77, return_tensors=\"pt\").to(device)\n",
    "                with torch.no_grad():\n",
    "                    text_out = pipe.text_encoder(**text_in)\n",
    "                if hasattr(text_out, \"last_hidden_state\"):\n",
    "                    text_embeddings = text_out.last_hidden_state\n",
    "                else:\n",
    "                    text_embeddings = text_out\n",
    "            else:\n",
    "                # fallback if no built-in text encoder\n",
    "                b_size = latents.shape[0]\n",
    "                text_embeddings = torch.randn((b_size, 77, 768), device=device)\n",
    "\n",
    "            # -------------------------\n",
    "            # 2) FACE & BODY EMBEDDINGS\n",
    "            # -------------------------\n",
    "            face_emb_list = []\n",
    "            body_emb_list = []\n",
    "            for b in range(latents.shape[0]):\n",
    "                img = pil_image[b] if isinstance(pil_image, list) else pil_image\n",
    "                bbox = detect_body_bbox_pil(img, query_text=\"a person.\")\n",
    "                body_emb = extract_body_embedding_pil(img, bbox)\n",
    "                body_emb_list.append(body_emb)\n",
    "\n",
    "                face_emb = extract_face_embedding_pil(img)\n",
    "                face_emb_list.append(face_emb)\n",
    "\n",
    "            face_emb_tensor = torch.cat(face_emb_list, dim=0).to(device)\n",
    "            body_emb_tensor = torch.cat(body_emb_list, dim=0).to(device)\n",
    "\n",
    "            # -------------------------\n",
    "            # 3) FORWARD PASS\n",
    "            # -------------------------\n",
    "            pred_latents = flux_wrapper(latents, t, text_embeddings, face_emb_tensor, body_emb_tensor)\n",
    "\n",
    "            # Diffusion MSE (mock)\n",
    "            diff_loss = diffusion_loss_fn(pred_latents, latents)\n",
    "\n",
    "            # -------------------------\n",
    "            # 4) DECODE & IDENTITY LOSS\n",
    "            # -------------------------\n",
    "            with torch.no_grad():\n",
    "                if pred_latents.dtype != pipe.vae.dtype:\n",
    "                    pred_latents = pred_latents.to(pipe.vae.dtype)\n",
    "                decoded = pipe.vae.decode(pred_latents).sample  # (B, 3, H, W)\n",
    "\n",
    "            face_loss_val = 0.0\n",
    "            body_loss_val = 0.0\n",
    "            b_sz = decoded.shape[0]\n",
    "\n",
    "            for b in range(b_sz):\n",
    "                # (3, H, W) -> PIL\n",
    "                img_np = decoded[b].detach().cpu().float().numpy()\n",
    "                img_min, img_max = img_np.min(), img_np.max()\n",
    "                img_np = (img_np - img_min) / (img_max - img_min + 1e-8)\n",
    "                img_np = (img_np * 255).astype(np.uint8)\n",
    "                img_np = np.transpose(img_np, (1, 2, 0))\n",
    "                pil_decoded = Image.fromarray(img_np)\n",
    "\n",
    "                gen_face_emb = extract_face_embedding_pil(pil_decoded)\n",
    "                f_loss = F.mse_loss(gen_face_emb, face_emb_tensor[b:b+1])\n",
    "                face_loss_val += f_loss.item()\n",
    "\n",
    "                dec_bbox = detect_body_bbox_pil(pil_decoded, query_text=\"a person.\")\n",
    "                gen_body_emb = extract_body_embedding_pil(pil_decoded, dec_bbox)\n",
    "                b_loss = F.mse_loss(gen_body_emb, body_emb_tensor[b:b+1])\n",
    "                body_loss_val += b_loss.item()\n",
    "\n",
    "            face_loss_val /= b_sz\n",
    "            body_loss_val /= b_sz\n",
    "\n",
    "            face_loss = torch.tensor(face_loss_val, device=device, requires_grad=True)\n",
    "            body_loss = torch.tensor(body_loss_val, device=device, requires_grad=True)\n",
    "\n",
    "            total_loss = diff_loss + lambda_face * face_loss + lambda_body * body_loss\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            total_loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if step_idx % 5 == 0:\n",
    "                print(f\"Epoch {epoch} | Step {step_idx} | \"\n",
    "                      f\"Diff={diff_loss.item():.4f} | Face={face_loss_val:.4f} | Body={body_loss_val:.4f} | \"\n",
    "                      f\"Total={total_loss.item():.4f}\")\n",
    "\n",
    "\n",
    "# =====================================================================\n",
    "# 9) MAIN / DEMO\n",
    "# =====================================================================\n",
    "def main():\n",
    "    # Example dataset\n",
    "    image_paths = [\"./example1.jpg\", \"./example2.jpg\"]\n",
    "    prompts = [\"Character in a futuristic city\", \"Character on the beach\"]\n",
    "    dataset = ExampleCharacterDataset(image_paths, prompts)\n",
    "    dataloader = DataLoader(dataset, batch_size=1, shuffle=True)\n",
    "\n",
    "    # Wrap the pipeline with new modules\n",
    "    flux_wrapper = FluxFrozenWrapper(\n",
    "        pipe,\n",
    "        embed_dim=768,\n",
    "        num_heads=8,\n",
    "        face_dim=512,\n",
    "        body_dim=768\n",
    "    ).to(device)\n",
    "\n",
    "    # Confirm submodules are frozen\n",
    "    for submodel in [pipe.transformer, pipe.text_encoder, pipe.text_encoder_2, pipe.vae]:\n",
    "        if submodel is not None:\n",
    "            for n, p in submodel.named_parameters():\n",
    "                assert not p.requires_grad, f\"Param {n} should be frozen!\"\n",
    "\n",
    "    # The only trainable parameters are in flux_wrapper\n",
    "    for n, p in flux_wrapper.named_parameters():\n",
    "        print(f\"{n} | requires_grad={p.requires_grad}\")\n",
    "\n",
    "    # 1) Run a quick training loop\n",
    "    train_identity_preservation(\n",
    "        flux_wrapper,\n",
    "        pipe,\n",
    "        dataloader,\n",
    "        epochs=1,\n",
    "        lr=1e-4,\n",
    "        lambda_face=1.0,\n",
    "        lambda_body=1.0\n",
    "    )\n",
    "\n",
    "    # 2) Demonstrate normal usage of the pipeline for generation\n",
    "    #    (You can do this after training, or skip if not needed)\n",
    "    print(\"\\nGenerating an image with the pipeline after training:\")\n",
    "    img = pipe(\n",
    "        prompt=\"woman\",\n",
    "        guidance_scale=2,\n",
    "        height=1024,\n",
    "        width=1024,\n",
    "        num_inference_steps=40,\n",
    "        generator=torch.Generator(\"cuda\").manual_seed(10)\n",
    "    ).images[0]\n",
    "\n",
    "    img.show()\n",
    "    img.save('after_training_generation.webp')\n",
    "    print(\"Saved 'after_training_generation.webp'\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
