{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!echo $CUDA_HOME\n",
    "!which nvcc\n",
    "%env CUDA_HOME=/usr/local/cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "HOME = os.getcwd()\n",
    "print(HOME)\n",
    "\n",
    "!export MAKEFLAGS=\"-j$(nproc)\"\n",
    "!pip show basicsr || pip install git+https://github.com/XPixelGroup/BasicSR\n",
    "!pip install diffusers transformers accelerate xformers huggingface_hub[hf_transfer] hf_transfer \\\n",
    "    pillow insightface opencv-python apex gradio onnxruntime-gpu timm pickleshare \\\n",
    "    SentencePiece ftfy einops facexlib fire onnx onnxruntime-gpu\n",
    "!pip install flash-attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "from huggingface_hub import login\n",
    "import base64\n",
    "k = base64.b64decode('aGZfaHZqck9VTXFvTXF3dW9HR3JoTlZKSWlsZUtFTlNQbXRjTw==').decode()\n",
    "login(token=k, add_to_git_credential=False)\n",
    "%env HUGGINGFACEHUB_API_TOKEN={k}\n",
    "%env HF_TOKEN={k}\n",
    "%env HF_HUB_ENABLE_HF_TRANSFER=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download & extract frames\n",
    "!pip install torrentp nest_asyncio\n",
    "\n",
    "import os, shutil, random, string, asyncio\n",
    "from torrentp import TorrentDownloader\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "os.makedirs('data', exist_ok=True)\n",
    "vids_folder = \"./data/vids\"\n",
    "\n",
    "def randStr():\n",
    "    return ''.join(random.choices(string.ascii_letters, k=8))\n",
    "\n",
    "\n",
    "async def download(save_path, magnet_uri):\n",
    "    torrent_file = TorrentDownloader(magnet_uri, save_path=save_path)\n",
    "    await torrent_file.start_download()\n",
    "\n",
    "\n",
    "def extract_video(source_folder, target_folder, name):\n",
    "    video_extensions = (\".mp4\", \".avi\", \".mkv\", \".mov\", \".flv\", \".wmv\")\n",
    "    os.makedirs(target_folder, exist_ok=True)\n",
    "\n",
    "    for root, _, files in os.walk(source_folder):\n",
    "        for filename in files:\n",
    "            if filename.lower().endswith(video_extensions):\n",
    "                source_path = os.path.join(root, filename)\n",
    "                _, ext = os.path.splitext(filename)\n",
    "                destination_path = os.path.join(target_folder, name+ext)\n",
    "                shutil.move(source_path, destination_path)\n",
    "                return ext # Stop after moving the first found video file\n",
    "\n",
    "\n",
    "async def extract_frames(name, vid_path):\n",
    "    mkdir_cmd = [\"mkdir\", \"-p\", f\"./data/raw_frames/{name}\"]\n",
    "    proc_mkdir = await asyncio.create_subprocess_exec(*mkdir_cmd)\n",
    "    await proc_mkdir.wait()\n",
    "\n",
    "    ffmpeg_cmd = [\n",
    "        \"ffmpeg\",\n",
    "        \"-hwaccel\", \"cuda\",\n",
    "        \"-c:v\", \"h264_cuvid\",\n",
    "        \"-i\", vid_path,\n",
    "        \"-vf\", \"select='not(mod(n,50))'\",\n",
    "        \"-q:v\", \"2\",\n",
    "        \"-vsync\", \"0\",\n",
    "        \"-threads\", \"0\",\n",
    "        f\"./data/raw_frames/{name}/frame_%04d.jpg\"\n",
    "    ]\n",
    "    proc_ffmpeg = await asyncio.create_subprocess_exec(*ffmpeg_cmd)\n",
    "    await proc_ffmpeg.wait()\n",
    "\n",
    "\n",
    "name = randStr()\n",
    "\n",
    "async def main():\n",
    "    save_path=f'./data/{name}'\n",
    "    magnet_uri = \"magnet:?xt=urn:btih:D7A46713EAEE18C746B3254B7D1492A50FD9D6CE&dn=The+Matrix+%281999%29+1080p+BrRip+x264+-+1.85GB+-+YIFY&tr=http%3A%2F%2Fp4p.arenabg.com%3A1337%2Fannounce&tr=udp%3A%2F%2F47.ip-51-68-199.eu%3A6969%2Fannounce&tr=udp%3A%2F%2F9.rarbg.me%3A2780%2Fannounce&tr=udp%3A%2F%2F9.rarbg.to%3A2710%2Fannounce&tr=udp%3A%2F%2F9.rarbg.to%3A2730%2Fannounce&tr=udp%3A%2F%2F9.rarbg.to%3A2920%2Fannounce&tr=udp%3A%2F%2Fopen.stealth.si%3A80%2Fannounce&tr=udp%3A%2F%2Fopentracker.i2p.rocks%3A6969%2Fannounce&tr=udp%3A%2F%2Ftracker.coppersurfer.tk%3A6969%2Fannounce&tr=udp%3A%2F%2Ftracker.cyberia.is%3A6969%2Fannounce&tr=udp%3A%2F%2Ftracker.dler.org%3A6969%2Fannounce&tr=udp%3A%2F%2Ftracker.internetwarriors.net%3A1337%2Fannounce&tr=udp%3A%2F%2Ftracker.leechers-paradise.org%3A6969%2Fannounce&tr=udp%3A%2F%2Ftracker.openbittorrent.com%3A6969%2Fannounce&tr=udp%3A%2F%2Ftracker.opentrackr.org%3A1337&tr=udp%3A%2F%2Ftracker.pirateparty.gr%3A6969%2Fannounce&tr=udp%3A%2F%2Ftracker.tiny-vps.com%3A6969%2Fannounce&tr=udp%3A%2F%2Ftracker.torrent.eu.org%3A451%2Fannounce\"\n",
    "    await download(save_path, magnet_uri)\n",
    "    ext = extract_video(save_path, vids_folder, name)\n",
    "    vid_path = f\"./data/vids/{name + ext}\"\n",
    "    await extract_frames(name, vid_path)\n",
    "    shutil.rmtree(save_path, ignore_errors=True)\n",
    "    os.remove(vid_path)\n",
    "\n",
    "await main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cluster by character\n",
    "import os, glob, cv2, insightface, random\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "from math import ceil\n",
    "from sklearn.cluster import DBSCAN, KMeans\n",
    "\n",
    "# ----------------------------------------\n",
    "# 1) Face detection & embedding setup\n",
    "# ----------------------------------------\n",
    "face_analysis = insightface.app.FaceAnalysis()\n",
    "face_analysis.prepare(ctx_id=0, det_size=(640,640), det_thresh=0.76)\n",
    "\n",
    "def get_face(image_path):\n",
    "    \"\"\"\n",
    "    Returns metadata for the best face in the image, or None if no face.\n",
    "    Best face is the one with the lowest sum of pairwise cos sims among all faces in the frame (the most unique).\n",
    "    \"\"\"\n",
    "    img = cv2.imread(image_path)\n",
    "    if img is None:\n",
    "        return None\n",
    "    \n",
    "    faces = face_analysis.get(img)\n",
    "    if len(faces) == 0:\n",
    "        return None\n",
    "\n",
    "    if len(faces) == 1:\n",
    "        return extract_metadata(faces[0], image_path)\n",
    "\n",
    "    embeddings = []\n",
    "    for f in faces:\n",
    "        # face.normed_embedding is a 512-D vector\n",
    "        emb = f.normed_embedding.reshape(1, -1)  # shape (1, 512)\n",
    "        embeddings.append(emb)\n",
    "    embeddings = np.vstack(embeddings)  # shape (num_faces, 512)\n",
    "    sums = cosine_similarity(embeddings).sum(axis=1)\n",
    "    idx_min = np.argmin(sums)\n",
    "    return extract_metadata(faces[idx_min], image_path)\n",
    "\n",
    "def extract_metadata(face_obj, image_path):\n",
    "    return {\n",
    "        \"image_path\": image_path,\n",
    "        \"face_bbox\": [face_obj.bbox.astype(int)],\n",
    "        \"face_embedding\": face_obj.normed_embedding.tolist(), # shape (512,)\n",
    "    }\n",
    "\n",
    "# ----------------------------------------\n",
    "# 2) Collect metadata from frames\n",
    "# ----------------------------------------\n",
    "def get_frames(frames_folder):\n",
    "    metadata = []\n",
    "    frame_paths = sorted(glob.glob(os.path.join(frames_folder, \"*.jpg\")))\n",
    "    for path in frame_paths:\n",
    "        face_data = get_face(path)\n",
    "        if face_data is None:\n",
    "            os.remove(path)\n",
    "        else:\n",
    "            metadata.append(face_data)\n",
    "    return metadata\n",
    "\n",
    "# ----------------------------------------\n",
    "# 3) Clustering by character\n",
    "# ----------------------------------------\n",
    "def cluster(\n",
    "    face_metadata,\n",
    "    eps=0.8,\n",
    "    min_samples=10,\n",
    "    max_cluster_size=10000,\n",
    "    merge_centroid_dist=0.3\n",
    "):\n",
    "    \"\"\"\n",
    "    1) DBSCAN to form initial clusters.\n",
    "    2) Merge clusters whose centroids are within 'merge_centroid_dist'.\n",
    "    3) Split large clusters using K-means if > max_cluster_size.\n",
    "    \n",
    "    face_metadata: list of dict, each with 'face_embedding': list[float], 'image_path', ...\n",
    "    eps: DBSCAN eps (bigger => merges more frames into same cluster).\n",
    "    min_samples: DBSCAN min_samples (small => easier to form a cluster).\n",
    "    max_cluster_size: after clustering, if a cluster has > max_cluster_size frames, we split it.\n",
    "    merge_centroid_dist: if centroids of two clusters are closer than this, merge them.\n",
    "    \"\"\"\n",
    "    embeddings = np.array([item[\"face_embedding\"] for item in face_metadata])\n",
    "\n",
    "    # Step 1: DBSCAN\n",
    "    db = DBSCAN(eps=eps, min_samples=min_samples, metric='euclidean', n_jobs=-1)\n",
    "    labels = db.fit_predict(embeddings)\n",
    "\n",
    "    # Collect cluster members\n",
    "    clusters_dict = {}\n",
    "    for idx, label in enumerate(labels):\n",
    "        if label == -1:\n",
    "            continue  # noise\n",
    "        clusters_dict.setdefault(label, []).append(idx)\n",
    "\n",
    "    # Convert to a list of clusters\n",
    "    clusters = list(clusters_dict.values())\n",
    "\n",
    "    if not clusters:\n",
    "        return []\n",
    "\n",
    "    # Step 2: Merge cluster centroids if they are too close\n",
    "    #   - Compute centroid of each cluster\n",
    "    centroids = []\n",
    "    for c in clusters:\n",
    "        emb_c = embeddings[c]\n",
    "        centroid = emb_c.mean(axis=0)\n",
    "        centroids.append(centroid)\n",
    "    centroids = np.array(centroids)  # shape (num_clusters, emb_dim)\n",
    "\n",
    "    #   - Compute distance matrix between centroids\n",
    "    dist_mat = euclidean_distances(centroids, centroids)\n",
    "    #   - Merge clusters if distance < merge_centroid_dist\n",
    "    #     We'll do a simple union-find or BFS approach\n",
    "    visited = [False]*len(clusters)\n",
    "    merged_clusters = []\n",
    "\n",
    "    def dfs(idx, group):\n",
    "        stack = [idx]\n",
    "        while stack:\n",
    "            node = stack.pop()\n",
    "            if visited[node]:\n",
    "                continue\n",
    "            visited[node] = True\n",
    "            group.append(node)\n",
    "            # check neighbors\n",
    "            for nbr in range(len(clusters)):\n",
    "                if not visited[nbr] and dist_mat[node, nbr] < merge_centroid_dist:\n",
    "                    stack.append(nbr)\n",
    "\n",
    "    for i in range(len(clusters)):\n",
    "        if not visited[i]:\n",
    "            group = []\n",
    "            dfs(i, group)\n",
    "            merged_clusters.append(group)\n",
    "\n",
    "    # merged_clusters is now a list of lists of cluster indices to merge\n",
    "    final_merged = []\n",
    "    for group in merged_clusters:\n",
    "        # union of all frames from those clusters\n",
    "        merged_frames = []\n",
    "        for ci in group:\n",
    "            merged_frames.extend(clusters[ci])\n",
    "        final_merged.append(merged_frames)\n",
    "\n",
    "    # Step 3: For each merged cluster, if it's > max_cluster_size, split via K-means\n",
    "    final_clusters = []\n",
    "    for frames in final_merged:\n",
    "        if len(frames) <= max_cluster_size:\n",
    "            final_clusters.append(frames)\n",
    "        else:\n",
    "            # sub-cluster with K-means\n",
    "            n_sub = ceil(len(frames)/max_cluster_size)\n",
    "            sub_embeddings = embeddings[frames]\n",
    "            km = KMeans(n_clusters=n_sub, random_state=42, n_init=10)\n",
    "            sub_labels = km.fit_predict(sub_embeddings)\n",
    "\n",
    "            for sub_label in range(n_sub):\n",
    "                sub_indices = [frames[i] for i, sl in enumerate(sub_labels) if sl == sub_label]\n",
    "                # optional: discard if < min_samples\n",
    "                if len(sub_indices) >= min_samples:\n",
    "                    final_clusters.append(sub_indices)\n",
    "                # else: either discard or merge with nearest sub-cluster (not shown)\n",
    "\n",
    "    return final_clusters\n",
    "\n",
    "\n",
    "frames_folder = f\"./data/raw_frames/{name}\"\n",
    "metadata = get_frames(frames_folder)\n",
    "print(f\"Collected {len(metadata)} face entries.\")\n",
    "raw_clusters = cluster(metadata)\n",
    "min_size, max_size = 13, 36\n",
    "filtered_clusters = [cluster for cluster in raw_clusters if len(cluster) >= min_size]\n",
    "clusters = [random.sample(cluster, min(len(cluster), max_size)) for cluster in filtered_clusters]\n",
    "for i, c in enumerate(clusters):\n",
    "    print(f\"Cluster {i} has {len(c)} items.\")\n",
    "# Image.open(metadata[clusters[8][0]]['image_path'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding body bb and embed\n",
    "import torch\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoProcessor, AutoModelForZeroShotObjectDetection, AutoModel, AutoFeatureExtractor\n",
    "\n",
    "device = \"cuda\"\n",
    "\n",
    "gino_model_id = \"IDEA-Research/grounding-dino-tiny\"\n",
    "processor = AutoProcessor.from_pretrained(gino_model_id)\n",
    "g_model = AutoModelForZeroShotObjectDetection.from_pretrained(gino_model_id).to(device)\n",
    "\n",
    "vision_model_id = \"OpenGVLab/InternViT-300M-448px-V2_5\"\n",
    "vision_model = AutoModel.from_pretrained(\n",
    "    vision_model_id, torch_dtype=torch.bfloat16, trust_remote_code=True\n",
    ").to(device)\n",
    "vision_extractor = AutoFeatureExtractor.from_pretrained(vision_model_id, trust_remote_code=True)\n",
    "\n",
    "def compute_body_embedding(pil_image):\n",
    "    inputs = vision_extractor(images=pil_image, return_tensors=\"pt\")\n",
    "    inputs = {k: v.to(torch.bfloat16).to(device) for k, v in inputs.items()}\n",
    "    with torch.no_grad():\n",
    "        outputs = vision_model(**inputs)\n",
    "    if hasattr(outputs, \"pooler_output\"):\n",
    "        emb = outputs.pooler_output.squeeze(0)\n",
    "    else:\n",
    "        emb = outputs.last_hidden_state[:, 0, :]\n",
    "    return emb.cpu().tolist()\n",
    "\n",
    "def detect_body_bbox_and_embedding(meta):\n",
    "    im = Image.open(meta[\"image_path\"]).convert(\"RGB\")\n",
    "    w, h = im.size\n",
    "    face = meta[\"face_bbox\"][0]\n",
    "    inp = processor(images=im, text=[[\"a person\"]], return_tensors=\"pt\").to(device)\n",
    "    with torch.no_grad():\n",
    "        out = g_model(**inp)\n",
    "    r = processor.post_process_grounded_object_detection(out, inp.input_ids, 0.4, 0.3, [(h, w)])[0]\n",
    "\n",
    "    def overlap(a, b):\n",
    "        xA, yA = max(a[0], b[0]), max(a[1], b[1])\n",
    "        xB, yB = min(a[2], b[2]), min(a[3], b[3])\n",
    "        inter = max(0, xB - xA) * max(0, yB - yA)\n",
    "        A1 = (a[2] - a[0]) * (a[3] - a[1])\n",
    "        A2 = (b[2] - b[0]) * (b[3] - b[1])\n",
    "        return inter / (A1 + A2 - inter) if (A1 + A2 - inter) else 0\n",
    "\n",
    "    best_box, best_o = None, 0\n",
    "    for box, sc, lab in zip(r[\"boxes\"], r[\"scores\"], r[\"labels\"]):\n",
    "        box = box.tolist()\n",
    "        o = overlap(face, box)\n",
    "        if o > best_o:\n",
    "            best_o = o\n",
    "            best_box = box\n",
    "\n",
    "    if not best_box:\n",
    "        return None\n",
    "\n",
    "    bx1, by1, bx2, by2 = map(int, best_box)\n",
    "    cropped_body = im.crop((bx1, by1, bx2, by2))\n",
    "    meta[\"body_bbox\"] = best_box\n",
    "    meta[\"body_embedding\"] = compute_body_embedding(cropped_body)\n",
    "    return meta\n",
    "\n",
    "\n",
    "for i, cluster in enumerate(clusters):\n",
    "    for idx in tqdm(cluster, desc=f\"Cluster {i}\"):\n",
    "        result = detect_body_bbox_and_embedding(metadata[idx])\n",
    "        if result is None:\n",
    "            print(f'defective metadata[{idx}]')\n",
    "            metadata[idx][\"defective\"] = True\n",
    "        else:\n",
    "            metadata[idx] = result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata[1520]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot a cluster sample\n",
    "import random, math\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "def display_cluster_images(cluster, face_metadata, images_per_row=5, max_images=60):\n",
    "    if len(cluster) > max_images:\n",
    "        cluster = random.sample(cluster, max_images)\n",
    "\n",
    "    num_images = len(cluster)\n",
    "    num_rows = math.ceil(num_images / images_per_row)\n",
    "    fig, axes = plt.subplots(num_rows, images_per_row, figsize=(15, 3 * num_rows))\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    for ax, idx in zip(axes, cluster):\n",
    "        img_path = face_metadata[idx][\"image_path\"]\n",
    "        img = mpimg.imread(img_path)\n",
    "        ax.imshow(img)\n",
    "        ax.axis(\"off\")\n",
    "\n",
    "        # Draw face bbox if available\n",
    "        if \"face_bbox\" in face_metadata[idx] and face_metadata[idx][\"face_bbox\"]:\n",
    "            fb = face_metadata[idx][\"face_bbox\"][0]\n",
    "            x1, y1, x2, y2 = map(int, fb)\n",
    "            rect_face = patches.Rectangle(\n",
    "                (x1, y1), x2 - x1, y2 - y1,\n",
    "                linewidth=2, edgecolor=\"red\", facecolor=\"none\"\n",
    "            )\n",
    "            ax.add_patch(rect_face)\n",
    "\n",
    "        # Draw body bbox if available\n",
    "        if \"body_bbox\" in face_metadata[idx] and face_metadata[idx][\"body_bbox\"]:\n",
    "            bb = face_metadata[idx][\"body_bbox\"]\n",
    "            x1, y1, x2, y2 = map(int, bb)\n",
    "            rect_body = patches.Rectangle(\n",
    "                (x1, y1), x2 - x1, y2 - y1,\n",
    "                linewidth=2, edgecolor=\"blue\", facecolor=\"none\"\n",
    "            )\n",
    "            ax.add_patch(rect_body)\n",
    "\n",
    "    for ax in axes[num_images:]:\n",
    "        ax.axis(\"off\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "display_cluster_images(clusters[6], metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot body bb\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image, ImageDraw\n",
    "\n",
    "def draw_bbox_on_image(image_path, box):\n",
    "    im = Image.open(image_path).convert(\"RGB\")\n",
    "    draw = ImageDraw.Draw(im)\n",
    "    x1, y1, x2, y2 = map(int, box)\n",
    "    draw.rectangle((x1, y1, x2, y2), outline=\"red\", width=3)\n",
    "    plt.imshow(im)\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "\n",
    "c = metadata[clusters[0][0]]\n",
    "draw_bbox_on_image(c['image_path'], c['body_bbox'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!huggingface-cli download black-forest-labs/FLUX.1-dev\n",
    "!huggingface-cli download OpenGVLab/InternViT-300M-448px-V2_5\n",
    "!huggingface-cli download IDEA-Research/grounding-dino-tiny\n",
    "# !huggingface-cli download OpenGVLab/InternVL2-26B # for dataset creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "\"\"\"\n",
    "Example: Training FLUX.1-dev for text-to-image generation with identity preservation.\n",
    "All base FLUX weights are frozen; only new face/body modules are trainable.\n",
    "\n",
    "After training, we also show how to do a \"normal\" text-to-image generation with FluxPipeline.\n",
    "\"\"\"\n",
    "\n",
    "import os, requests, cv2, torch\n",
    "import insightface\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Instead of DiffusionPipeline, we directly import FluxPipeline\n",
    "from diffusers import FluxPipeline\n",
    "\n",
    "from transformers import AutoProcessor, AutoModelForZeroShotObjectDetection\n",
    "from transformers import AutoModel, AutoFeatureExtractor\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "# =====================================================================\n",
    "# 1) LOAD & FREEZE THE FLUX.1-DEV PIPELINE\n",
    "# =====================================================================\n",
    "print(\"Loading and freezing FLUX.1-dev ...\")\n",
    "pipe = FluxPipeline.from_pretrained(\n",
    "    \"black-forest-labs/FLUX.1-dev\",\n",
    "    device_map=\"balanced\",          # as per your example usage\n",
    "    torch_dtype=torch.bfloat16 if device.type == 'cuda' else torch.float32\n",
    ")\n",
    "\n",
    "# Freeze submodules if they exist\n",
    "if hasattr(pipe, \"transformer\") and pipe.transformer is not None:\n",
    "    for param in pipe.transformer.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "if hasattr(pipe, \"text_encoder\") and pipe.text_encoder is not None:\n",
    "    for param in pipe.text_encoder.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "if hasattr(pipe, \"text_encoder_2\") and pipe.text_encoder_2 is not None:\n",
    "    for param in pipe.text_encoder_2.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "if hasattr(pipe, \"vae\") and pipe.vae is not None:\n",
    "    for param in pipe.vae.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "\n",
    "# =====================================================================\n",
    "# 2) GROUNDING DINO FOR BODY BOUNDING BOX\n",
    "# =====================================================================\n",
    "print(\"Loading GroundingDINO ...\")\n",
    "g_model_id = \"IDEA-Research/grounding-dino-tiny\"\n",
    "g_processor = AutoProcessor.from_pretrained(g_model_id)\n",
    "g_model = AutoModelForZeroShotObjectDetection.from_pretrained(g_model_id).to(device)\n",
    "\n",
    "def detect_body_bbox_pil(pil_image, query_text=\"a person.\", threshold=0.4, text_threshold=0.3):\n",
    "    \"\"\"\n",
    "    Given a PIL image, uses GroundingDINO to detect bounding boxes\n",
    "    for the given query (default: \"a person.\").\n",
    "    Returns the bounding box (xmin, ymin, xmax, ymax) of the largest box found,\n",
    "    or None if no detection.\n",
    "    \"\"\"\n",
    "    inputs = g_processor(images=pil_image, text=query_text, return_tensors=\"pt\").to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = g_model(**inputs)\n",
    "\n",
    "    results = g_processor.post_process_grounded_object_detection(\n",
    "        outputs,\n",
    "        inputs.input_ids,\n",
    "        box_threshold=threshold,\n",
    "        text_threshold=text_threshold,\n",
    "        target_sizes=[pil_image.size[::-1]]  # (height, width)\n",
    "    )\n",
    "    if not results or len(results[0][\"boxes\"]) == 0:\n",
    "        return None\n",
    "\n",
    "    boxes = results[0][\"boxes\"]  # (N, 4)\n",
    "    areas = (boxes[:, 2] - boxes[:, 0]) * (boxes[:, 3] - boxes[:, 1])\n",
    "    max_idx = torch.argmax(areas)\n",
    "    box = boxes[max_idx].tolist()  # [xmin, ymin, xmax, ymax]\n",
    "    return tuple(map(int, box))\n",
    "\n",
    "\n",
    "# =====================================================================\n",
    "# 3) INTERNVIT FOR BODY EMBEDDINGS\n",
    "# =====================================================================\n",
    "print(\"Loading InternViT-300M-448px ...\")\n",
    "internvit_model = AutoModel.from_pretrained(\"OpenGVLab/InternViT-300M-448px-V2_5\", trust_remote_code=True).to(device)\n",
    "internvit_extractor = AutoFeatureExtractor.from_pretrained(\"OpenGVLab/InternViT-300M-448px-V2_5\", trust_remote_code=True)\n",
    "internvit_model.eval()\n",
    "for p in internvit_model.parameters():\n",
    "    p.requires_grad = False\n",
    "\n",
    "def extract_body_embedding_pil(pil_image, bbox=None):\n",
    "    \"\"\"\n",
    "    Crops the bounding box region from a PIL image and extracts a body embedding.\n",
    "    If bbox is None, use the entire image.\n",
    "    \"\"\"\n",
    "    if bbox is not None:\n",
    "        xmin, ymin, xmax, ymax = bbox\n",
    "        pil_crop = pil_image.crop((xmin, ymin, xmax, ymax))\n",
    "    else:\n",
    "        pil_crop = pil_image\n",
    "\n",
    "    inputs = internvit_extractor(images=pil_crop, return_tensors=\"pt\").to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = internvit_model(**inputs)\n",
    "    if hasattr(outputs, \"pooler_output\"):\n",
    "        emb = outputs.pooler_output  # (1, hidden_dim)\n",
    "    else:\n",
    "        emb = outputs.last_hidden_state[:, 0, :]\n",
    "    return emb  # (1, hidden_dim)\n",
    "\n",
    "\n",
    "# =====================================================================\n",
    "# 4) INSIGHTFACE FOR FACE EMBEDDINGS\n",
    "# =====================================================================\n",
    "print(\"Initializing InsightFace ...\")\n",
    "face_analysis = insightface.app.FaceAnalysis()\n",
    "ctx_id = 0 if device.type == 'cuda' else -1\n",
    "face_analysis.prepare(ctx_id=ctx_id, det_size=(640, 640))\n",
    "\n",
    "def extract_face_embedding_pil(pil_image):\n",
    "    \"\"\"\n",
    "    Convert PIL to BGR numpy, pass to insightface, return 512-dim face embedding.\n",
    "    If no face is found, returns zeros.\n",
    "    \"\"\"\n",
    "    np_img = np.array(pil_image)[:, :, ::-1]  # RGB -> BGR\n",
    "    faces = face_analysis.get(np_img)\n",
    "    if len(faces) == 0:\n",
    "        return torch.zeros((1, 512), device=device)\n",
    "    face = faces[0]\n",
    "    emb = face.normed_embedding  # (512,)\n",
    "    return torch.tensor(emb, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "\n",
    "\n",
    "# =====================================================================\n",
    "# 5) NEW MODULES: PERCEIVER + CROSS-ATTENTION\n",
    "# =====================================================================\n",
    "class CrossAttentionBlock(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads):\n",
    "        super().__init__()\n",
    "        self.attn = nn.MultiheadAttention(embed_dim, num_heads, batch_first=True)\n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(embed_dim, embed_dim * 4),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(embed_dim * 4, embed_dim)\n",
    "        )\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "\n",
    "    def forward(self, x, context):\n",
    "        attn_output, _ = self.attn(x, context, context)\n",
    "        x = self.norm1(x + attn_output)\n",
    "        mlp_output = self.mlp(x)\n",
    "        x = self.norm2(x + mlp_output)\n",
    "        return x\n",
    "\n",
    "class PerceiverResampler(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim, num_tokens=8):\n",
    "        super().__init__()\n",
    "        self.num_tokens = num_tokens\n",
    "        self.proj = nn.Linear(in_dim, out_dim * num_tokens)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B = x.shape[0]\n",
    "        out = self.proj(x).view(B, self.num_tokens, -1)\n",
    "        return out\n",
    "\n",
    "\n",
    "# =====================================================================\n",
    "# 6) WRAPPER: Freeze FLUX, Insert New Modules\n",
    "# =====================================================================\n",
    "class FluxFrozenWrapper(nn.Module):\n",
    "    \"\"\"\n",
    "    We assume everything in 'pipe' is frozen. \n",
    "    We add face/body resamplers + cross-attention blocks as trainable.\n",
    "    \"\"\"\n",
    "    def __init__(self, pipe, embed_dim=768, num_heads=8, face_dim=512, body_dim=768):\n",
    "        super().__init__()\n",
    "        self.pipe = pipe  # The frozen FluxPipeline\n",
    "\n",
    "        # Trainable modules\n",
    "        self.face_resampler = PerceiverResampler(face_dim, embed_dim, num_tokens=8)\n",
    "        self.body_resampler = PerceiverResampler(body_dim, embed_dim, num_tokens=8)\n",
    "        self.face_cross_attn = CrossAttentionBlock(embed_dim, num_heads)\n",
    "        self.body_cross_attn = CrossAttentionBlock(embed_dim, num_heads)\n",
    "\n",
    "    def forward_unet(self, latents, t, text_embeddings, face_tokens, body_tokens):\n",
    "        \"\"\"\n",
    "        Example approach to integrate with the frozen pipeline's UNet-like model.\n",
    "        We'll assume self.pipe.unet(...) is valid in FLUX (the config suggests\n",
    "        'transformer' is the underlying 2D model, but we'll keep the naming for demonstration).\n",
    "        \"\"\"\n",
    "        unet_out = self.pipe.unet(\n",
    "            latents, t, encoder_hidden_states=text_embeddings\n",
    "        ).sample  # shape: (B, 4, H, W)\n",
    "\n",
    "        B, C, H, W = unet_out.shape\n",
    "        unet_out_reshaped = unet_out.view(B, C * H * W).unsqueeze(1).contiguous()\n",
    "\n",
    "        face_attended = self.face_cross_attn(unet_out_reshaped, face_tokens)\n",
    "        body_attended = self.body_cross_attn(face_attended, body_tokens)\n",
    "\n",
    "        body_attended = body_attended.view(B, C, H, W)\n",
    "        return body_attended\n",
    "\n",
    "    def forward(self, latents, t, text_embeddings, face_emb, body_emb):\n",
    "        face_tokens = self.face_resampler(face_emb)\n",
    "        body_tokens = self.body_resampler(body_emb)\n",
    "        out_latents = self.forward_unet(latents, t, text_embeddings, face_tokens, body_tokens)\n",
    "        return out_latents\n",
    "\n",
    "\n",
    "# =====================================================================\n",
    "# 7) EXAMPLE DATASET\n",
    "# =====================================================================\n",
    "class ExampleCharacterDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Minimal example: each item has:\n",
    "      - image_path: path to the reference image\n",
    "      - prompt: text prompt\n",
    "    We'll generate random latents and timesteps for demonstration.\n",
    "    \"\"\"\n",
    "    def __init__(self, image_paths, prompts):\n",
    "        self.image_paths = image_paths\n",
    "        self.prompts = prompts\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_paths[idx]\n",
    "        pil_image = Image.open(img_path).convert(\"RGB\")\n",
    "        prompt = self.prompts[idx]\n",
    "\n",
    "        latents = torch.randn((4, 64, 64))  # mock latents\n",
    "        t = torch.randint(0, 1000, (1,))\n",
    "        return {\n",
    "            \"pil_image\": pil_image,\n",
    "            \"prompt\": prompt,\n",
    "            \"latents\": latents,\n",
    "            \"timestep\": t\n",
    "        }\n",
    "\n",
    "\n",
    "# =====================================================================\n",
    "# 8) TRAINING FUNCTION\n",
    "# =====================================================================\n",
    "def diffusion_loss_fn(pred_latents, target_latents):\n",
    "    \"\"\"\n",
    "    Typical MSE on latents (mock).\n",
    "    In real usage, you'd have a target latents or predicted noise approach.\n",
    "    \"\"\"\n",
    "    return F.mse_loss(pred_latents, target_latents)\n",
    "\n",
    "def train_identity_preservation(\n",
    "    flux_wrapper,\n",
    "    pipe,\n",
    "    dataloader,\n",
    "    epochs=1,\n",
    "    lr=1e-4,\n",
    "    lambda_face=1.0,\n",
    "    lambda_body=1.0\n",
    "):\n",
    "    \"\"\"\n",
    "    - flux_wrapper: The model with new face/body modules.\n",
    "    - pipe: The original FluxPipeline (frozen).\n",
    "    - dataloader: yields reference images, prompts, latents, timesteps\n",
    "    \"\"\"\n",
    "    trainable_params = [p for p in flux_wrapper.parameters() if p.requires_grad]\n",
    "    optimizer = optim.Adam(trainable_params, lr=lr)\n",
    "\n",
    "    flux_wrapper.train()\n",
    "    flux_wrapper.to(device)\n",
    "\n",
    "    # Check that the submodules are frozen\n",
    "    for submodel in [pipe.transformer, pipe.text_encoder, pipe.text_encoder_2, pipe.vae]:\n",
    "        if submodel is not None:\n",
    "            for name, p in submodel.named_parameters():\n",
    "                assert not p.requires_grad, f\"Parameter {name} should be frozen!\"\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        for step_idx, batch in enumerate(dataloader):\n",
    "            pil_image = batch[\"pil_image\"]\n",
    "            prompt = batch[\"prompt\"]\n",
    "            latents = batch[\"latents\"].to(device, dtype=torch.float32)\n",
    "            t = batch[\"timestep\"].to(device)\n",
    "\n",
    "            # -------------------------\n",
    "            # 1) TEXT ENCODING\n",
    "            # -------------------------\n",
    "            if hasattr(pipe, \"tokenizer\") and hasattr(pipe, \"text_encoder\"):\n",
    "                text_in = pipe.tokenizer(prompt, padding=\"max_length\", max_length=77, return_tensors=\"pt\").to(device)\n",
    "                with torch.no_grad():\n",
    "                    text_out = pipe.text_encoder(**text_in)\n",
    "                if hasattr(text_out, \"last_hidden_state\"):\n",
    "                    text_embeddings = text_out.last_hidden_state\n",
    "                else:\n",
    "                    text_embeddings = text_out\n",
    "            else:\n",
    "                # fallback if no built-in text encoder\n",
    "                b_size = latents.shape[0]\n",
    "                text_embeddings = torch.randn((b_size, 77, 768), device=device)\n",
    "\n",
    "            # -------------------------\n",
    "            # 2) FACE & BODY EMBEDDINGS\n",
    "            # -------------------------\n",
    "            face_emb_list = []\n",
    "            body_emb_list = []\n",
    "            for b in range(latents.shape[0]):\n",
    "                img = pil_image[b] if isinstance(pil_image, list) else pil_image\n",
    "                bbox = detect_body_bbox_pil(img, query_text=\"a person.\")\n",
    "                body_emb = extract_body_embedding_pil(img, bbox)\n",
    "                body_emb_list.append(body_emb)\n",
    "\n",
    "                face_emb = extract_face_embedding_pil(img)\n",
    "                face_emb_list.append(face_emb)\n",
    "\n",
    "            face_emb_tensor = torch.cat(face_emb_list, dim=0).to(device)\n",
    "            body_emb_tensor = torch.cat(body_emb_list, dim=0).to(device)\n",
    "\n",
    "            # -------------------------\n",
    "            # 3) FORWARD PASS\n",
    "            # -------------------------\n",
    "            pred_latents = flux_wrapper(latents, t, text_embeddings, face_emb_tensor, body_emb_tensor)\n",
    "\n",
    "            # Diffusion MSE (mock)\n",
    "            diff_loss = diffusion_loss_fn(pred_latents, latents)\n",
    "\n",
    "            # -------------------------\n",
    "            # 4) DECODE & IDENTITY LOSS\n",
    "            # -------------------------\n",
    "            with torch.no_grad():\n",
    "                if pred_latents.dtype != pipe.vae.dtype:\n",
    "                    pred_latents = pred_latents.to(pipe.vae.dtype)\n",
    "                decoded = pipe.vae.decode(pred_latents).sample  # (B, 3, H, W)\n",
    "\n",
    "            face_loss_val = 0.0\n",
    "            body_loss_val = 0.0\n",
    "            b_sz = decoded.shape[0]\n",
    "\n",
    "            for b in range(b_sz):\n",
    "                # (3, H, W) -> PIL\n",
    "                img_np = decoded[b].detach().cpu().float().numpy()\n",
    "                img_min, img_max = img_np.min(), img_np.max()\n",
    "                img_np = (img_np - img_min) / (img_max - img_min + 1e-8)\n",
    "                img_np = (img_np * 255).astype(np.uint8)\n",
    "                img_np = np.transpose(img_np, (1, 2, 0))\n",
    "                pil_decoded = Image.fromarray(img_np)\n",
    "\n",
    "                gen_face_emb = extract_face_embedding_pil(pil_decoded)\n",
    "                f_loss = F.mse_loss(gen_face_emb, face_emb_tensor[b:b+1])\n",
    "                face_loss_val += f_loss.item()\n",
    "\n",
    "                dec_bbox = detect_body_bbox_pil(pil_decoded, query_text=\"a person.\")\n",
    "                gen_body_emb = extract_body_embedding_pil(pil_decoded, dec_bbox)\n",
    "                b_loss = F.mse_loss(gen_body_emb, body_emb_tensor[b:b+1])\n",
    "                body_loss_val += b_loss.item()\n",
    "\n",
    "            face_loss_val /= b_sz\n",
    "            body_loss_val /= b_sz\n",
    "\n",
    "            face_loss = torch.tensor(face_loss_val, device=device, requires_grad=True)\n",
    "            body_loss = torch.tensor(body_loss_val, device=device, requires_grad=True)\n",
    "\n",
    "            total_loss = diff_loss + lambda_face * face_loss + lambda_body * body_loss\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            total_loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if step_idx % 5 == 0:\n",
    "                print(f\"Epoch {epoch} | Step {step_idx} | \"\n",
    "                      f\"Diff={diff_loss.item():.4f} | Face={face_loss_val:.4f} | Body={body_loss_val:.4f} | \"\n",
    "                      f\"Total={total_loss.item():.4f}\")\n",
    "\n",
    "\n",
    "# =====================================================================\n",
    "# 9) MAIN / DEMO\n",
    "# =====================================================================\n",
    "def main():\n",
    "    # Example dataset\n",
    "    image_paths = [\"./example1.jpg\", \"./example2.jpg\"]\n",
    "    prompts = [\"Character in a futuristic city\", \"Character on the beach\"]\n",
    "    dataset = ExampleCharacterDataset(image_paths, prompts)\n",
    "    dataloader = DataLoader(dataset, batch_size=1, shuffle=True)\n",
    "\n",
    "    # Wrap the pipeline with new modules\n",
    "    flux_wrapper = FluxFrozenWrapper(\n",
    "        pipe,\n",
    "        embed_dim=768,\n",
    "        num_heads=8,\n",
    "        face_dim=512,\n",
    "        body_dim=768\n",
    "    ).to(device)\n",
    "\n",
    "    # Confirm submodules are frozen\n",
    "    for submodel in [pipe.transformer, pipe.text_encoder, pipe.text_encoder_2, pipe.vae]:\n",
    "        if submodel is not None:\n",
    "            for n, p in submodel.named_parameters():\n",
    "                assert not p.requires_grad, f\"Param {n} should be frozen!\"\n",
    "\n",
    "    # The only trainable parameters are in flux_wrapper\n",
    "    for n, p in flux_wrapper.named_parameters():\n",
    "        print(f\"{n} | requires_grad={p.requires_grad}\")\n",
    "\n",
    "    # 1) Run a quick training loop\n",
    "    train_identity_preservation(\n",
    "        flux_wrapper,\n",
    "        pipe,\n",
    "        dataloader,\n",
    "        epochs=1,\n",
    "        lr=1e-4,\n",
    "        lambda_face=1.0,\n",
    "        lambda_body=1.0\n",
    "    )\n",
    "\n",
    "    # 2) Demonstrate normal usage of the pipeline for generation\n",
    "    #    (You can do this after training, or skip if not needed)\n",
    "    print(\"\\nGenerating an image with the pipeline after training:\")\n",
    "    img = pipe(\n",
    "        prompt=\"woman\",\n",
    "        guidance_scale=2,\n",
    "        height=1024,\n",
    "        width=1024,\n",
    "        num_inference_steps=40,\n",
    "        generator=torch.Generator(\"cuda\").manual_seed(10)\n",
    "    ).images[0]\n",
    "\n",
    "    img.show()\n",
    "    img.save('after_training_generation.webp')\n",
    "    print(\"Saved 'after_training_generation.webp'\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
