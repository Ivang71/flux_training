{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install diffusers transformers accelerate xformers huggingface_hub\n",
    "!pip install requests pillow insightface opencv-python apex gradio diffusers onnx onnxruntime-gpu onnxruntime timm \\\n",
    "    SentencePiece git+https://github.com/XPixelGroup/BasicSR ftfy einops facexlib fire\n",
    "!pip install torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cu118\n",
    "!pip install \"git+https://github.com/IDEA-Research/GroundingDINO.git\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# Setting up HF transfer\n",
    "from huggingface_hub import login\n",
    "import base64\n",
    "t = 'aGZfaHZqck9VTXFvTXF3dW9HR3JoTlZKSWlsZUtFTlNQbXRjTw=='\n",
    "k = base64.b64decode(t.encode()).decode()\n",
    "login(token=k, add_to_git_credential=False)\n",
    "%env HUGGINGFACEHUB_API_TOKEN={k}\n",
    "%env HF_TOKEN={k}\n",
    "%env HF_HUB_ENABLE_HF_TRANSFER=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!huggingface-cli download black-forest-labs/FLUX.1-dev\n",
    "!huggingface-cli download OpenGVLab/InternViT-300M-448px\n",
    "!huggingface-cli download IDEA-Research/grounding-dino-base\n",
    "!huggingface-cli download OpenGVLab/InternVL2-26B # for dataset creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Example: Training FLUX.1-dev for text-to-image generation with identity preservation.\n",
    "All base FLUX weights are frozen; only new face/body modules are trainable.\n",
    "\n",
    "Requirements:\n",
    "    pip install diffusers transformers accelerate xformers huggingface_hub\n",
    "    pip install insightface opencv-python\n",
    "    pip install timm  # for InternViT\n",
    "    pip install torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cu118  # if using CUDA\n",
    "    pip install requests pillow  # for image loading\n",
    "    pip install \"git+https://github.com/IDEA-Research/GroundingDINO.git\"\n",
    "\"\"\"\n",
    "\n",
    "import os, requests, cv2, torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from diffusers import DiffusionPipeline\n",
    "from transformers import AutoProcessor, AutoModelForZeroShotObjectDetection\n",
    "import insightface\n",
    "from transformers import AutoModel, AutoFeatureExtractor\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# =====================================================================\n",
    "# 1) LOAD & FREEZE THE FLUX.1-DEV PIPELINE\n",
    "# =====================================================================\n",
    "print(\"Loading and freezing FLUX.1-dev ...\")\n",
    "flux_pipeline = DiffusionPipeline.from_pretrained(\n",
    "    \"black-forest-labs/FLUX.1-dev\",\n",
    "    torch_dtype=torch.bfloat16 if device.type == 'cuda' else torch.float32\n",
    ").to(device)\n",
    "\n",
    "if hasattr(flux_pipeline, \"unet\"):\n",
    "    for param in flux_pipeline.unet.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "if hasattr(flux_pipeline, \"vae\"):\n",
    "    for param in flux_pipeline.vae.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "if hasattr(flux_pipeline, \"text_encoder\"):\n",
    "    for param in flux_pipeline.text_encoder.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "# =====================================================================\n",
    "# 2) GROUNDING DINO FOR BODY BOUNDING BOX\n",
    "#    (Using your snippet-based approach)\n",
    "# =====================================================================\n",
    "print(\"Loading GroundingDINO (IDEA-Research/grounding-dino-base) ...\")\n",
    "g_model_id = \"IDEA-Research/grounding-dino-base\"\n",
    "g_processor = AutoProcessor.from_pretrained(g_model_id)\n",
    "g_model = AutoModelForZeroShotObjectDetection.from_pretrained(g_model_id).to(device)\n",
    "\n",
    "def detect_body_bbox_pil(pil_image, query_text=\"a person.\", box_threshold=0.4, text_threshold=0.3):\n",
    "    \"\"\"\n",
    "    Given a PIL image, uses GroundingDINO to detect bounding boxes\n",
    "    for the given query (default: \"a person.\").\n",
    "    Returns the bounding box (xmin, ymin, xmax, ymax) of the largest box found,\n",
    "    or None if no detection.\n",
    "    \"\"\"\n",
    "    # Convert to device\n",
    "    inputs = g_processor(images=pil_image, text=query_text, return_tensors=\"pt\").to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = g_model(**inputs)\n",
    "\n",
    "    # Post-process to get final boxes\n",
    "    results = g_processor.post_process_grounded_object_detection(\n",
    "        outputs,\n",
    "        inputs.input_ids,\n",
    "        box_threshold=box_threshold,\n",
    "        text_threshold=text_threshold,\n",
    "        target_sizes=[pil_image.size[::-1]]  # (height, width)\n",
    "    )\n",
    "    if not results or len(results[0][\"boxes\"]) == 0:\n",
    "        return None\n",
    "\n",
    "    # Find largest bounding box\n",
    "    boxes = results[0][\"boxes\"]  # (N, 4) in xyxy format\n",
    "    areas = (boxes[:, 2] - boxes[:, 0]) * (boxes[:, 3] - boxes[:, 1])\n",
    "    max_idx = torch.argmax(areas)\n",
    "    box = boxes[max_idx].tolist()  # [xmin, ymin, xmax, ymax]\n",
    "    return tuple(map(int, box))\n",
    "\n",
    "\n",
    "# =====================================================================\n",
    "# 3) INTERNVIT FOR BODY EMBEDDINGS\n",
    "# =====================================================================\n",
    "print(\"Loading InternViT-300M-448px ...\")\n",
    "internvit_model = AutoModel.from_pretrained(\"OpenGVLab/InternViT-300M-448px\", trust_remote_code=True).to(device)\n",
    "internvit_extractor = AutoFeatureExtractor.from_pretrained(\"OpenGVLab/InternViT-300M-448px\", trust_remote_code=True)\n",
    "internvit_model.eval()\n",
    "for p in internvit_model.parameters():\n",
    "    p.requires_grad = False\n",
    "\n",
    "def extract_body_embedding_pil(pil_image, bbox=None):\n",
    "    \"\"\"\n",
    "    Crops the bounding box region from a PIL image and extracts a body embedding.\n",
    "    If bbox is None, use the entire image.\n",
    "    \"\"\"\n",
    "    if bbox is not None:\n",
    "        xmin, ymin, xmax, ymax = bbox\n",
    "        pil_crop = pil_image.crop((xmin, ymin, xmax, ymax))\n",
    "    else:\n",
    "        pil_crop = pil_image\n",
    "\n",
    "    inputs = internvit_extractor(images=pil_crop, return_tensors=\"pt\").to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = internvit_model(**inputs)\n",
    "    # Typically, we use outputs.pooler_output or the CLS token\n",
    "    if hasattr(outputs, \"pooler_output\"):\n",
    "        emb = outputs.pooler_output  # (1, hidden_dim)\n",
    "    else:\n",
    "        emb = outputs.last_hidden_state[:, 0, :]  # CLS token\n",
    "    return emb  # shape (1, hidden_dim)\n",
    "\n",
    "\n",
    "# =====================================================================\n",
    "# 4) INSIGHTFACE FOR FACE EMBEDDINGS\n",
    "# =====================================================================\n",
    "print(\"Initializing InsightFace ...\")\n",
    "face_analysis = insightface.app.FaceAnalysis()\n",
    "ctx_id = 0 if device.type == 'cuda' else -1\n",
    "face_analysis.prepare(ctx_id=ctx_id, det_size=(640, 640))\n",
    "\n",
    "def extract_face_embedding_pil(pil_image):\n",
    "    \"\"\"\n",
    "    Convert PIL to BGR numpy, pass to insightface, return 512-dim face embedding.\n",
    "    If no face is found, returns zeros.\n",
    "    \"\"\"\n",
    "    np_img = np.array(pil_image)[:, :, ::-1]  # RGB -> BGR\n",
    "    faces = face_analysis.get(np_img)\n",
    "    if len(faces) == 0:\n",
    "        return torch.zeros((1, 512), device=device)\n",
    "    face = faces[0]\n",
    "    emb = face.normed_embedding  # (512,)\n",
    "    return torch.tensor(emb, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "\n",
    "\n",
    "# =====================================================================\n",
    "# 5) NEW MODULES: PERCEIVER + CROSS-ATTENTION\n",
    "# =====================================================================\n",
    "class CrossAttentionBlock(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads):\n",
    "        super().__init__()\n",
    "        self.attn = nn.MultiheadAttention(embed_dim, num_heads, batch_first=True)\n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(embed_dim, embed_dim * 4),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(embed_dim * 4, embed_dim)\n",
    "        )\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "\n",
    "    def forward(self, x, context):\n",
    "        # x, context: (B, seq_len, embed_dim)\n",
    "        attn_output, _ = self.attn(x, context, context)\n",
    "        x = self.norm1(x + attn_output)\n",
    "        mlp_output = self.mlp(x)\n",
    "        x = self.norm2(x + mlp_output)\n",
    "        return x\n",
    "\n",
    "class PerceiverResampler(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim, num_tokens=8):\n",
    "        super().__init__()\n",
    "        self.num_tokens = num_tokens\n",
    "        self.proj = nn.Linear(in_dim, out_dim * num_tokens)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: (B, in_dim) -> (B, num_tokens, out_dim)\n",
    "        \"\"\"\n",
    "        B = x.shape[0]\n",
    "        out = self.proj(x).view(B, self.num_tokens, -1)\n",
    "        return out\n",
    "\n",
    "\n",
    "# =====================================================================\n",
    "# 6) WRAPPER: Freeze FLUX, Insert New Modules\n",
    "# =====================================================================\n",
    "class FluxFrozenWrapper(nn.Module):\n",
    "    \"\"\"\n",
    "    We assume flux_pipeline has a .unet, .vae, .tokenizer, .text_encoder, etc.\n",
    "    Everything in flux_pipeline is frozen. \n",
    "    We add face/body resamplers + cross-attention blocks as trainable.\n",
    "    \"\"\"\n",
    "    def __init__(self, flux_pipe, embed_dim=768, num_heads=8, face_dim=512, body_dim=768):\n",
    "        super().__init__()\n",
    "        self.flux_pipe = flux_pipe  # already frozen externally\n",
    "\n",
    "        # Trainable modules\n",
    "        self.face_resampler = PerceiverResampler(face_dim, embed_dim, num_tokens=8)\n",
    "        self.body_resampler = PerceiverResampler(body_dim, embed_dim, num_tokens=8)\n",
    "        self.face_cross_attn = CrossAttentionBlock(embed_dim, num_heads)\n",
    "        self.body_cross_attn = CrossAttentionBlock(embed_dim, num_heads)\n",
    "\n",
    "    def forward_unet(self, latents, t, text_embeddings, face_tokens, body_tokens):\n",
    "        \"\"\"\n",
    "        Example approach to integrate with the frozen UNet.\n",
    "        This depends heavily on the actual FLUX.1-dev code.\n",
    "        We'll do a simplified example using .unet(...) from diffusers.\n",
    "        \"\"\"\n",
    "        # 1) Standard UNet forward\n",
    "        #    encoder_hidden_states = text_embeddings\n",
    "        unet_out = self.flux_pipe.unet(\n",
    "            latents, t, encoder_hidden_states=text_embeddings\n",
    "        ).sample  # shape: (B, 4, H, W) if it's SD-like\n",
    "\n",
    "        # 2) Flatten/reshape for cross-attn\n",
    "        B, C, H, W = unet_out.shape\n",
    "        # Make a seq_len dimension: (B, seq_len, embed_dim)\n",
    "        # We'll pretend embed_dim = C, seq_len = H*W for demonstration:\n",
    "        unet_out_reshaped = unet_out.view(B, C*H*W).unsqueeze(1).contiguous()\n",
    "\n",
    "        # 3) Face cross-attention\n",
    "        face_attended = self.face_cross_attn(unet_out_reshaped, face_tokens)\n",
    "\n",
    "        # 4) Body cross-attention\n",
    "        body_attended = self.body_cross_attn(face_attended, body_tokens)\n",
    "\n",
    "        # 5) Reshape back\n",
    "        body_attended = body_attended.view(B, C, H, W)\n",
    "        return body_attended\n",
    "\n",
    "    def forward(self, latents, t, text_embeddings, face_emb, body_emb):\n",
    "        \"\"\"\n",
    "        latents: (B, 4, H, W)  # e.g. stable-diffusion-like latents\n",
    "        t: (B,) or scalar diffusion timestep\n",
    "        text_embeddings: (B, seq_len, embed_dim)\n",
    "        face_emb: (B, 512)\n",
    "        body_emb: (B, 768)\n",
    "        \"\"\"\n",
    "        # Resample face/body\n",
    "        face_tokens = self.face_resampler(face_emb)  # (B, 8, embed_dim)\n",
    "        body_tokens = self.body_resampler(body_emb)  # (B, 8, embed_dim)\n",
    "\n",
    "        # Forward through the UNet with extra cross-attn\n",
    "        out_latents = self.forward_unet(latents, t, text_embeddings, face_tokens, body_tokens)\n",
    "        return out_latents\n",
    "\n",
    "\n",
    "# =====================================================================\n",
    "# 7) EXAMPLE DATASET\n",
    "# =====================================================================\n",
    "class ExampleCharacterDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Minimal example: each item has:\n",
    "      - image_path: path to the reference image\n",
    "      - prompt: text prompt\n",
    "    We'll generate random latents and timesteps for demonstration.\n",
    "    \"\"\"\n",
    "    def __init__(self, image_paths, prompts):\n",
    "        self.image_paths = image_paths\n",
    "        self.prompts = prompts\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_paths[idx]\n",
    "        pil_image = Image.open(img_path).convert(\"RGB\")\n",
    "        prompt = self.prompts[idx]\n",
    "\n",
    "        latents = torch.randn((4, 64, 64))  # mock latents\n",
    "        t = torch.randint(0, 1000, (1,))    # mock diffusion timestep\n",
    "        return {\n",
    "            \"pil_image\": pil_image,\n",
    "            \"prompt\": prompt,\n",
    "            \"latents\": latents,\n",
    "            \"timestep\": t\n",
    "        }\n",
    "\n",
    "\n",
    "# =====================================================================\n",
    "# 8) TRAINING FUNCTION\n",
    "# =====================================================================\n",
    "def diffusion_loss_fn(pred_latents, target_latents):\n",
    "    \"\"\"\n",
    "    Typical MSE on latents (mock).\n",
    "    In real usage, you'd have a target latents or predicted noise approach.\n",
    "    \"\"\"\n",
    "    return F.mse_loss(pred_latents, target_latents)\n",
    "\n",
    "def train_identity_preservation(\n",
    "    flux_wrapper,\n",
    "    flux_pipeline,\n",
    "    dataloader,\n",
    "    epochs=1,\n",
    "    lr=1e-4,\n",
    "    lambda_face=1.0,\n",
    "    lambda_body=1.0\n",
    "):\n",
    "    \"\"\"\n",
    "    - flux_wrapper: The model with new face/body modules.\n",
    "    - flux_pipeline: The original pipeline (frozen).\n",
    "    - dataloader: yields reference images, prompts, latents, timesteps\n",
    "    - We do a simple loop computing:\n",
    "        total_loss = diffusion_loss + lambda_face * face_loss + lambda_body * body_loss\n",
    "      Only the new modules update.\n",
    "    \"\"\"\n",
    "    # Collect only trainable params (the new modules)\n",
    "    trainable_params = [p for p in flux_wrapper.parameters() if p.requires_grad]\n",
    "    optimizer = optim.Adam(trainable_params, lr=lr)\n",
    "\n",
    "    flux_wrapper.train()\n",
    "    flux_wrapper.to(device)\n",
    "\n",
    "    # Confirm the base pipeline is frozen\n",
    "    for name, p in flux_pipeline.named_parameters():\n",
    "        assert p.requires_grad is False, f\"Parameter {name} should be frozen!\"\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        for step_idx, batch in enumerate(dataloader):\n",
    "            pil_image = batch[\"pil_image\"]\n",
    "            prompt = batch[\"prompt\"]\n",
    "            latents = batch[\"latents\"].to(device, dtype=torch.float32)\n",
    "            t = batch[\"timestep\"].to(device)\n",
    "\n",
    "            # -------------------------\n",
    "            # 1) TEXT ENCODING\n",
    "            # -------------------------\n",
    "            if hasattr(flux_pipeline, \"tokenizer\") and hasattr(flux_pipeline, \"text_encoder\"):\n",
    "                text_in = flux_pipeline.tokenizer(prompt, padding=\"max_length\", max_length=77, return_tensors=\"pt\").to(device)\n",
    "                with torch.no_grad():\n",
    "                    text_out = flux_pipeline.text_encoder(**text_in)\n",
    "                if hasattr(text_out, \"last_hidden_state\"):\n",
    "                    text_embeddings = text_out.last_hidden_state  # (B, seq_len, embed_dim)\n",
    "                else:\n",
    "                    text_embeddings = text_out\n",
    "            else:\n",
    "                # Fallback if FLUX.1-dev pipeline doesn't have a built-in text encoder\n",
    "                # We'll just use random embeddings\n",
    "                b_size = latents.shape[0]\n",
    "                text_embeddings = torch.randn((b_size, 77, 768), device=device)\n",
    "\n",
    "            # -------------------------\n",
    "            # 2) FACE & BODY EMBEDDINGS\n",
    "            # -------------------------\n",
    "            face_emb_list = []\n",
    "            body_emb_list = []\n",
    "            for b in range(latents.shape[0]):\n",
    "                img = pil_image[b] if isinstance(pil_image, list) else pil_image\n",
    "                # detect bounding box\n",
    "                bbox = detect_body_bbox_pil(img, query_text=\"a person.\")\n",
    "                # extract body emb\n",
    "                body_emb = extract_body_embedding_pil(img, bbox)\n",
    "                body_emb_list.append(body_emb)\n",
    "\n",
    "                # extract face emb\n",
    "                face_emb = extract_face_embedding_pil(img)\n",
    "                face_emb_list.append(face_emb)\n",
    "\n",
    "            face_emb_tensor = torch.cat(face_emb_list, dim=0).to(device)  # (B, 512)\n",
    "            body_emb_tensor = torch.cat(body_emb_list, dim=0).to(device)  # (B, 768)\n",
    "\n",
    "            # -------------------------\n",
    "            # 3) FORWARD PASS\n",
    "            # -------------------------\n",
    "            pred_latents = flux_wrapper(\n",
    "                latents,\n",
    "                t,\n",
    "                text_embeddings,\n",
    "                face_emb_tensor,\n",
    "                body_emb_tensor\n",
    "            )\n",
    "\n",
    "            # For demonstration, let's assume target latents = latents\n",
    "            diff_loss = diffusion_loss_fn(pred_latents, latents)\n",
    "\n",
    "            # -------------------------\n",
    "            # 4) DECODE & IDENTITY LOSS\n",
    "            # -------------------------\n",
    "            # Convert pred_latents -> image, to compute face/body embeddings on the output\n",
    "            # Typically with Stable Diffusion-like pipelines, you'd do:\n",
    "            with torch.no_grad():\n",
    "                # flux_pipeline.vae expects half precision if it's in half\n",
    "                if pred_latents.dtype != flux_pipeline.vae.dtype:\n",
    "                    pred_latents = pred_latents.to(flux_pipeline.vae.dtype)\n",
    "                decoded = flux_pipeline.vae.decode(pred_latents).sample  # (B, 3, H, W)\n",
    "\n",
    "            # For each image in the batch, compute face/body embedding\n",
    "            face_loss_val = 0.0\n",
    "            body_loss_val = 0.0\n",
    "            b_sz = decoded.shape[0]\n",
    "            for b in range(b_sz):\n",
    "                # Convert to PIL for consistency\n",
    "                # (3, H, W) -> (H, W, 3)\n",
    "                img_np = decoded[b].detach().cpu().float().numpy()\n",
    "                img_min, img_max = img_np.min(), img_np.max()\n",
    "                img_np = (img_np - img_min) / (img_max - img_min + 1e-8)  # [0,1]\n",
    "                img_np = (img_np * 255).astype(np.uint8)\n",
    "                img_np = np.transpose(img_np, (1,2,0))  # HWC\n",
    "                pil_decoded = Image.fromarray(img_np)\n",
    "\n",
    "                # Face embedding\n",
    "                gen_face_emb = extract_face_embedding_pil(pil_decoded)\n",
    "                f_loss = F.mse_loss(gen_face_emb, face_emb_tensor[b:b+1])\n",
    "                face_loss_val += f_loss.item()\n",
    "\n",
    "                # Body embedding\n",
    "                dec_bbox = detect_body_bbox_pil(pil_decoded, query_text=\"a person.\")\n",
    "                gen_body_emb = extract_body_embedding_pil(pil_decoded, dec_bbox)\n",
    "                b_loss = F.mse_loss(gen_body_emb, body_emb_tensor[b:b+1])\n",
    "                body_loss_val += b_loss.item()\n",
    "\n",
    "            face_loss_val /= b_sz\n",
    "            body_loss_val /= b_sz\n",
    "\n",
    "            # Turn them into tensors that require grad (so they backprop into cross-attn)\n",
    "            face_loss = torch.tensor(face_loss_val, device=device, requires_grad=True)\n",
    "            body_loss = torch.tensor(body_loss_val, device=device, requires_grad=True)\n",
    "\n",
    "            total_loss = diff_loss + lambda_face * face_loss + lambda_body * body_loss\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            total_loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if step_idx % 5 == 0:\n",
    "                print(f\"Epoch {epoch} | Step {step_idx} | \"\n",
    "                      f\"Diff={diff_loss.item():.4f} | Face={face_loss_val:.4f} | Body={body_loss_val:.4f} | \"\n",
    "                      f\"Total={total_loss.item():.4f}\")\n",
    "\n",
    "\n",
    "# =====================================================================\n",
    "# 9) MAIN / DEMO\n",
    "# =====================================================================\n",
    "def main():\n",
    "    # Minimal dataset: adapt to your actual data\n",
    "    image_paths = [\"./example1.jpg\", \"./example2.jpg\"]  # images of your character\n",
    "    prompts = [\"Character in a futuristic city\", \"Character on the beach\"]\n",
    "\n",
    "    dataset = ExampleCharacterDataset(image_paths, prompts)\n",
    "    dataloader = DataLoader(dataset, batch_size=1, shuffle=True)\n",
    "\n",
    "    # Wrap FLUX with new modules\n",
    "    flux_wrapper = FluxFrozenWrapper(\n",
    "        flux_pipeline,\n",
    "        embed_dim=768,\n",
    "        num_heads=8,\n",
    "        face_dim=512,\n",
    "        body_dim=768\n",
    "    ).to(device)\n",
    "\n",
    "    # Ensure base FLUX is still frozen\n",
    "    for n, p in flux_pipeline.named_parameters():\n",
    "        assert not p.requires_grad, f\"Param {n} should be frozen!\"\n",
    "    # The only trainable parameters are in flux_wrapper's new modules\n",
    "    for n, p in flux_wrapper.named_parameters():\n",
    "        print(f\"{n} | requires_grad={p.requires_grad}\")\n",
    "\n",
    "    # Train\n",
    "    train_identity_preservation(\n",
    "        flux_wrapper,\n",
    "        flux_pipeline,\n",
    "        dataloader,\n",
    "        epochs=1,\n",
    "        lr=1e-4,\n",
    "        lambda_face=1.0,\n",
    "        lambda_body=1.0\n",
    "    )\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
