{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc53df01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # one-time setup\n",
    "# # installing cmake\n",
    "# !wget https://github.com/Kitware/CMake/releases/download/v3.31.4/cmake-3.31.4-linux-x86_64.sh\n",
    "# !chmod +x cmake-3.31.4-linux-x86_64.sh\n",
    "# !sudo ./cmake-3.31.4-linux-x86_64.sh --prefix=/usr/local --skip-license \n",
    "# !rm cmake-3.31.4-linux-x86_64.sh\n",
    "# !cmake --version\n",
    "\n",
    "\n",
    "# # installing cuda\n",
    "# # !sudo apt remove --purge '^nvidia-.*' -y\n",
    "# !sudo apt autoremove -y\n",
    "# !wget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu$(lsb_release -rs | tr -d '.')/x86_64/cuda-keyring_1.0-1_all.deb\n",
    "# !sudo dpkg -i cuda-keyring_1.0-1_all.deb\n",
    "# # !rm cuda-keyring_1.0-1_all.deb\n",
    "# !sudo apt update\n",
    "# !sudo apt install cuda -y\n",
    "# !echo 'export PATH=/usr/local/cuda/bin:$PATH' >> ~/.bashrc\n",
    "# !echo 'export LD_LIBRARY_PATH=/usr/local/cuda/lib64:$LD_LIBRARY_PATH' >> ~/.bashrc\n",
    "# !source ~/.bashrc\n",
    "\n",
    "\n",
    "# # stable_diffusion_cpp_python\n",
    "# !sudo apt install build-essential g++-11 ninja-build\n",
    "# !sudo update-alternatives --install /usr/bin/gcc gcc /usr/bin/gcc-11 100\n",
    "# !sudo update-alternatives --install /usr/bin/g++ g++ /usr/bin/g++-11 100\n",
    "# !CMAKE_ARGS=\"-DCUDA_ARCH=sm_89 -DSD_CUDA=ON -DSD_FLASH_ATTN=ON\" pip install stable-diffusion-cpp-python --upgrade --force-reinstall --no-cache-dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9d58d46",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip uninstall numpy torch flash-attn transformers accelerate optimum auto-gptq huggingface-hub huggingface_hub[hf_transfer] pickleshare safetensors \\\n",
    "                pymongo json5 diffusers protobuf peft hf_transfer supabase xformers \"xfuser[diffusers,flash-attn]\" -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10c60232-c71f-42a4-b876-bae76231a1b2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%env MAKEFLAGS=\"-j$(nproc)\"\n",
    "!pip install numpy torch\n",
    "!pip install flash-attn --no-build-isolation\n",
    "!pip install transformers accelerate optimum auto-gptq huggingface-hub huggingface_hub[hf_transfer] pickleshare safetensors \\\n",
    "                pymongo json5 diffusers protobuf peft hf_transfer supabase --upgrade\n",
    "\n",
    "!pip install xformers \"xfuser[diffusers,flash-attn]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ed62d2c",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "!pip install python-dotenv supabase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81079794",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()  # This loads the variables from .env into os.environ\n",
    "\n",
    "import os\n",
    "from supabase import create_client, Client\n",
    "\n",
    "url: str = os.environ.get(\"SUPABASE_URL\")\n",
    "key: str = os.environ.get(\"SUPABASE_KEY\")\n",
    "supabase: Client = create_client(url, key)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fab83e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# Setting up HF transfer\n",
    "from huggingface_hub import login\n",
    "import base64\n",
    "t = 'aGZfaHZqck9VTXFvTXF3dW9HR3JoTlZKSWlsZUtFTlNQbXRjTw=='\n",
    "k = base64.b64decode(t.encode()).decode()\n",
    "login(token=k, add_to_git_credential=True)\n",
    "%env HUGGINGFACEHUB_API_TOKEN={k}\n",
    "%env HF_TOKEN={k}\n",
    "%env HF_HUB_ENABLE_HF_TRANSFER=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff745242",
   "metadata": {},
   "outputs": [],
   "source": [
    "# in terminal pip install hf_transfer\n",
    "# !huggingface-cli download black-forest-labs/FLUX.1-dev ae.safetensors\n",
    "!huggingface-cli download comfyanonymous/flux_text_encoders clip_l.safetensors\n",
    "!huggingface-cli download comfyanonymous/flux_text_encoders t5xxl_fp16.safetensors\n",
    "# !huggingface-cli download kudzueye/boreal-flux-dev-v2 boreal-v2.safetensors\n",
    "# !huggingface-cli download kudzueye/Boreal boreal-flux-dev-lora-v04_1000_steps.safetensors\n",
    "# !huggingface-cli download Shakker-Labs/FLUX.1-dev-LoRA-AntiBlur FLUX-dev-lora-AntiBlur.safetensors\n",
    "\n",
    "!huggingface-cli download black-forest-labs/FLUX.1-dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d76e06c6-5440-43aa-b8d7-2ad2d53bb93e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "cd /workspace/dtback\n",
    "git clone -b sd3 https://github.com/kohya-ss/sd-scripts.git\n",
    "cd sd-scripts\n",
    "pip install --upgrade -r requirements.txt\n",
    "mkdir dataset\n",
    "mkdir loras\n",
    "\n",
    "cat > dataset_1024.toml << EOF\n",
    "[[datasets]]\n",
    "resolution = 1024\n",
    "batch_size = 4\n",
    "keep_tokens = 2\n",
    "\n",
    "  [[datasets.subsets]]\n",
    "  image_dir = '/workspace/dtback/sd-scripts/dataset'\n",
    "  class_tokens = 'a woman'\n",
    "  num_repeats=10\n",
    "  face_crop_aug_range=[1.0, 3.0]\n",
    "EOF\n",
    "\n",
    "cd /workspace/dtback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b704092-c817-4223-8c52-0b0dd7561f44",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install triton bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a64a751-edf7-4b2b-84ef-11aadabac775",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "fpath=\"/root/.cache/huggingface/hub/models--black-forest-labs--FLUX.1-dev/snapshots/0ef5fff789c832c5c7f4e127f94c8b54bbcced44\"\n",
    "epath=\"/root/.cache/huggingface/hub/models--comfyanonymous--flux_text_encoders/snapshots/6af2a98e3f615bdfa612fbd85da93d1ed5f69ef5\"\n",
    "\n",
    "accelerate launch --mixed_precision bf16 --num_cpu_threads_per_process 10 sd-scripts/flux_train_network.py \\\n",
    "--pretrained_model_name_or_path \"${fpath}/flux1-dev.safetensors\" --clip_l \"${epath}/clip_l.safetensors\" --t5xxl \"${epath}/t5xxl_fp16.safetensors\" \\\n",
    "--ae \"${fpath}/ae.safetensors\" --cache_latents_to_disk --save_model_as safetensors --sdpa --persistent_data_loader_workers \\\n",
    "--max_data_loader_n_workers 6 --seed 42 --gradient_checkpointing --mixed_precision bf16 --save_precision bf16 \\\n",
    "--network_module networks.lora_flux --network_dim 4 --network_train_unet_only \\\n",
    "--optimizer_type adamw8bit --learning_rate 1e-4 \\\n",
    "--cache_text_encoder_outputs --cache_text_encoder_outputs_to_disk \\\n",
    "--highvram --max_train_epochs 4 --save_every_n_epochs 1 --dataset_config sd-scripts/dataset_1024.toml \\\n",
    "--output_dir sd-scripts/loras --output_name wm \\\n",
    "--timestep_sampling shift --discrete_flow_shift 3.1582 --model_prediction_type raw --guidance_scale 1.0 \\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ac60914-5b53-4378-ad0b-0efcb948b681",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# test the lora\n",
    "import torch\n",
    "from diffusers import FluxPipeline\n",
    "\n",
    "if \"pipe\" not in globals():\n",
    "    pipe = FluxPipeline.from_pretrained(\"black-forest-labs/FLUX.1-dev\", device_map=\"balanced\", torch_dtype=torch.bfloat16)\n",
    "    pipe.load_lora_weights(\"/workspace/dtback/sd-scripts/loras\", weight_name=\"wm.safetensors\", adapter_name=\"wm\")\n",
    "    pipe.set_adapters(\"wm\", 1)\n",
    "    pipe.fuse_lora()\n",
    "base_prompt = \"photo of a woman\"\n",
    "img = pipe(prompt=base_prompt, guidance_scale=0, height=1280, width=1024, num_inference_steps=30,\n",
    "           generator=torch.Generator(\"cuda\").manual_seed(1000)).images[0]\n",
    "img.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e45b9a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = pipe(\n",
    "    prompt=\"photo of a woman riding a horse\",\n",
    "    width=width,\n",
    "    height=height,\n",
    "    num_inference_steps=20,\n",
    "    generator=torch.Generator(\"cuda\").manual_seed(42)\n",
    ").images[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5717453c",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from diffusers import FluxPriorReduxPipeline, FluxPipeline\n",
    "from diffusers.utils import load_image\n",
    "\n",
    "if \"pipe\" not in globals():\n",
    "    pipe_prior_redux = FluxPriorReduxPipeline.from_pretrained(\"black-forest-labs/FLUX.1-Redux-dev\", torch_dtype=torch.bfloat16).to(\"cuda\")\n",
    "    pipe = FluxPipeline.from_pretrained(\"black-forest-labs/FLUX.1-dev\", torch_dtype=torch.bfloat16).to(\"cuda\")\n",
    "\n",
    "image = load_image(\"./base.webp\")\n",
    "\n",
    "pipe_prior_output = pipe_prior_redux(image)\n",
    "img_prompt_embeds = pipe_prior_output[\"prompt_embeds\"]\n",
    "\n",
    "text_prompt = \"woman sitting on an porch\"\n",
    "text_prompt_embeds, pooled_text_embeds, _ = pipe.encode_prompt(\n",
    "    prompt=text_prompt,\n",
    "    prompt_2=text_prompt,\n",
    "    device=\"cuda\",\n",
    "    num_images_per_prompt=1,\n",
    ")\n",
    "\n",
    "# Interpolate the text embeddings to match the image embeddings' sequence length.\n",
    "target_length = img_prompt_embeds.shape[1]  # e.g. 1241\n",
    "if text_prompt_embeds.shape[1] != target_length:\n",
    "    # Transpose to (B, embed_dim, seq_length)\n",
    "    text_prompt_embeds = text_prompt_embeds.transpose(1, 2)\n",
    "    # Interpolate along the sequence dimension.\n",
    "    text_prompt_embeds = F.interpolate(text_prompt_embeds, size=target_length, mode='linear', align_corners=False)\n",
    "    # Transpose back to (B, seq_length, embed_dim)\n",
    "    text_prompt_embeds = text_prompt_embeds.transpose(1, 2)\n",
    "\n",
    "# Blend the embeddings.\n",
    "alpha = 0.83\n",
    "blended_embeds = alpha * text_prompt_embeds + (1 - alpha) * img_prompt_embeds\n",
    "\n",
    "# For pooled embeddings, you might choose to use one source directly or blend if they are compatible.\n",
    "# Here, we simply use the pooled text embeddings.\n",
    "blended_pooled = pooled_text_embeds\n",
    "\n",
    "# Now call the Flux pipeline using the blended embeddings.\n",
    "img = pipe(\n",
    "    guidance_scale=0,\n",
    "    num_inference_steps=20,\n",
    "    generator=torch.Generator(\"cuda\").manual_seed(0),\n",
    "    prompt_embeds=blended_embeds,\n",
    "    pooled_prompt_embeds=blended_pooled,\n",
    "    **{k: v for k, v in pipe_prior_output.items() if k not in [\"prompt_embeds\", \"pooled_prompt_embeds\"]}\n",
    ").images[0]\n",
    "\n",
    "img.show()\n",
    "img.save(\"redux_blended.webp\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16c38e89",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from diffusers import FluxPipeline\n",
    "\n",
    "if \"pipe\" not in globals():\n",
    "    pipe = FluxPipeline.from_pretrained(\"black-forest-labs/FLUX.1-dev\", device_map=\"balanced\", torch_dtype=torch.bfloat16)\n",
    "\n",
    "prompt= \"a portrait of a futuristic cyborg with striking facial features\"\n",
    "img = pipe(prompt=prompt, guidance_scale=0, height=1280, width=1024, num_inference_steps=20,\n",
    "           generator=torch.Generator(\"cuda\").manual_seed(42)).images[0]\n",
    "img.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6bf1a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate the base\n",
    "import torch\n",
    "from diffusers import FluxPipeline\n",
    "\n",
    "if \"pipe\" not in globals():\n",
    "    pipe = FluxPipeline.from_pretrained(\"black-forest-labs/FLUX.1-dev\", device_map=\"balanced\", torch_dtype=torch.bfloat16)\n",
    "    pipe.load_lora_weights(\"kudzueye/Boreal\", weight_name=\"boreal-flux-dev-lora-v04_1000_steps.safetensors\", adapter_name=\"boreal\")\n",
    "    pipe.load_lora_weights(\"/workspace/dtback/models/img/loras/sameface.safetensors\", adapter_name=\"sameface\")\n",
    "    pipe.set_adapters([\"boreal\", \"sameface\"], [0.7, -0.2])\n",
    "    pipe.fuse_lora()\n",
    "base_prompt = \"photo of a young redhead woman with unique facial features wearing underwear\"\n",
    "img = pipe(prompt=base_prompt, guidance_scale=0, height=1280, width=1024, num_inference_steps=30,\n",
    "           generator=torch.Generator(\"cuda\").manual_seed(1000)).images[0]\n",
    "img.show()\n",
    "img.save(f'out.webp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b6337fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate the rest\n",
    "import torch\n",
    "from diffusers import FluxImg2ImgPipeline\n",
    "from diffusers.utils import load_image\n",
    "\n",
    "if \"pipe\" not in globals():\n",
    "    pipe = FluxImg2ImgPipeline.from_pretrained(\"black-forest-labs/FLUX.1-dev\", torch_dtype=torch.bfloat16).to(\"cuda\")\n",
    "    pipe.load_lora_weights(\"kudzueye/Boreal\", weight_name=\"boreal-flux-dev-lora-v04_1000_steps.safetensors\", adapter_name=\"boreal\")\n",
    "    pipe.set_adapters(\"boreal\", 0.15)\n",
    "    pipe.fuse_lora()\n",
    "\n",
    "\n",
    "init_image = load_image(\"./1920_0.6.webp\")\n",
    "width, height = init_image.size\n",
    "prompt = \"photo of a woman in a snowy city\"\n",
    "\n",
    "img = pipe(\n",
    "    prompt=prompt, image=init_image, num_inference_steps=60, strength=0.6, guidance_scale=7.0,\n",
    "    generator=torch.Generator(\"cuda\").manual_seed(1000), width=width, height=height,\n",
    ").images[0]\n",
    "img.show()\n",
    "img.save('img.webp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "603d5b4b-4783-4b97-9fbc-479d84c58a4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !rm -rf ~/.cache/huggingface\n",
    "# !rm -rf mongo_persistence && mkdir mongo_persistence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45981784-8d79-4c1b-84a7-f775d9592144",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Downloading models\n",
    "\n",
    "# !huggingface-cli download mlabonne/Meta-Llama-3.1-8B-Instruct-abliterated-GGUF meta-llama-3.1-8b-instruct-abliterated.Q8_0.gguf --local-dir ~/models/llms\n",
    "# !huggingface-cli download mlabonne/Meta-Llama-3.1-8B-Instruct-abliterated-GGUF meta-llama-3.1-8b-instruct-abliterated.Q4_K_M.gguf --local-dir ~/models/llms\n",
    "# !huggingface-cli download mlabonne/Meta-Llama-3.1-8B-Instruct-abliterated-GGUF meta-llama-3.1-8b-instruct-abliterated.Q2_K.gguf --local-dir ~/models/llms\n",
    "\n",
    "# !huggingface-cli download bartowski/Llama-3.3-70B-Instruct-abliterated-GGUF Llama-3.3-70B-Instruct-abliterated-IQ2_S.gguf --local-dir ~/models/llms\n",
    "\n",
    "# !huggingface-cli download mradermacher/Qwen2.5-32B-Instruct-abliterated-i1-GGUF Qwen2.5-32B-Instruct-abliterated.i1-Q4_K_S.gguf --local-dir ~/models/llms\n",
    "# !huggingface-cli download mradermacher/Qwen2.5-32B-Instruct-abliterated-i1-GGUF Qwen2.5-32B-Instruct-abliterated.i1-IQ3_M.gguf --local-dir ~/models/llms\n",
    "\n",
    "# !huggingface-cli download mradermacher/DeepSeek-R1-Distill-Qwen-32B-abliterated-i1-GGUF DeepSeek-R1-Distill-Qwen-32B-abliterated.i1-Q4_K_M.gguf --local-dir ~/models/llms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9796bb06-5295-4f66-8985-3c500d509c82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up LLama.cpp\n",
    "\n",
    "# !sudo apt install libcurl4-openssl-dev\n",
    "# !apt-cache policy nvidia-cuda-toolkit\n",
    "# !sudo apt remove nvidia-cuda-toolkit && sudo apt autoremove\n",
    "\n",
    "# !git clone https://github.com/ggerganov/llama.cpp --depth 1 --single-branch\n",
    "\n",
    "# %cd /teamspace/studios/this_studio/llama.cpp\n",
    "# !cmake -B build -DGGML_CUDA=ON -DGGML_CUDA_F16=ON -DGGML_CUDA_FA_ALL_QUANTS=ON\n",
    "# !cmake --build build --config Release -j 32\n",
    "# %cd ~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "710b8f60-22bf-404d-ac4d-1c4f80eb5f2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing llama.cpp tools\n",
    "\n",
    "# !.~/llama.cpp/build/bin/llama-cli\n",
    "%cd ~/llama.cpp/build/bin\n",
    "\n",
    "# !./llama-cli -m ~/llama.gguf -h\n",
    "# !./llama-cli -m ~/models/llama_6.gguf --gpu-layers 18 --ctx-size 131072 --batch-size 65536 -p \"I just \" --predict 1024 -no-cnv\n",
    "\n",
    "!./llama-server -m ~/models/llama_4.gguf --port 8000 --gpu-layers 33 --ctx-size 65536 --batch-size 65536\n",
    "\n",
    "# !./llama-bench -m ~/models/llama_4.gguf\n",
    "\n",
    "# !./llama-cli -m ~/models/llama_4.gguf --gpu-layers 33 --ctx-size 94208 --batch-size 65536 -p \"Sometimes \" --predict 128 -no-cnv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9969fd70-e374-481d-95a2-4823d8f82caa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# short test generation\n",
    "print(requests.post(\"http://127.0.0.1:8000/v1/chat/completions\", json={\"messages\": [\n",
    "    {\n",
    "      \"role\": \"system\",\n",
    "      \"content\": \"You are a professional sexual romance writer.\",\n",
    "    },\n",
    "    {\n",
    "      \"role\": \"user\",\n",
    "      \"content\": \"make up a very explicit sexual scene\",          \n",
    "    }\n",
    "]}).json()['choices'][0]['message']['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6dd0576-21b2-4a93-9233-fa668ddeb01e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" server start command\n",
    "~/llama.cpp/build/bin/llama-server -m ~/models/llms/llama_8.gguf --port 8000 --gpu-layers 33 --ctx-size 131072 \\\n",
    "--cache-type-k q8_0 --cache-type-v q8_0 --flash-attn\n",
    "\"\"\"\n",
    "\n",
    "\"\"\" start with grammar\n",
    "~/llama.cpp/build/bin/llama-server -m ~/models/llms/llama_4.gguf --port 8000 --gpu-layers 33 --ctx-size 131072 \\\n",
    "--cache-type-k q4_0 --cache-type-v q4_0 --flash-attn --grammar-file ~/generation/1_bots_text.gbnf\n",
    "~/llama.cpp/build/bin/llama-server -m ~/models/llms/llama_4.gguf --port 8000 --gpu-layers 33 --ctx-size 131072 \\\n",
    "--cache-type-k q4_0 --cache-type-v q4_0 --flash-attn --grammar-file ~/generation/2_add_img_prompts_to_bots.gbnf\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "949be372-3c4d-42d8-86a8-90dd6714095e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# installing docker\n",
    "\n",
    "# !sudo apt install postgresql postgresql-contrib # installing postgres\n",
    "# !sudo systemctl enable postgresql\n",
    "# !sudo systemctl start postgresql\n",
    "\n",
    "# !sudo apt install -y apt-transport-https ca-certificates curl software-properties-common\n",
    "# !yes | curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo gpg --dearmor -o /usr/share/keyrings/docker-archive-keyring.gpg\n",
    "# !sudo add-apt-repository \\\"deb [arch=amd64] https://download.docker.com/linux/ubuntu focal stable\\\"\n",
    "# !apt-cache policy docker-ce\n",
    "# !sudo apt install docker-ce\n",
    "# !sudo systemctl status docker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "256762fa-ab1a-4b69-9370-5c7d239e4de3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# start mongo\n",
    "!wget -qO- https://www.mongodb.org/static/pgp/server-8.0.asc | sudo tee /etc/apt/trusted.gpg.d/server-8.0.asc\n",
    "!echo \"deb [ arch=amd64,arm64 ] https://repo.mongodb.org/apt/ubuntu focal/mongodb-org/8.0 multiverse\" | sudo tee /etc/apt/sources.list.d/mongodb-org-8.0.list\n",
    "!sudo apt-get update\n",
    "!sudo apt-get install -y mongodb-mongosh\n",
    "\n",
    "!docker run --name mongodb -p 27017:27017 -v ~/mongo_persistence:/data/db -d mongo\n",
    "# mongosh --port 27017\n",
    "# docker stop mongodb && docker rm mongodb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b4702c0-4a91-4d98-936d-d16d9fa1e5cf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## grammar builder https://grammar.intrinsiclabs.ai\n",
    "# !python3 generation/1_bots_text.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4859244-214b-464d-a9ef-4a28f4950659",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# stable-diffusion.cpp\n",
    "\n",
    "# %cd ~\n",
    "# !rm -rf stable-diffusion.cpp\n",
    "# !git clone --recursive https://github.com/leejet/stable-diffusion.cpp\n",
    "# %cd ~/stable-diffusion.cpp\n",
    "# !git pull origin master\n",
    "# !git submodule init\n",
    "# !git submodule update\n",
    "# %cd ~\n",
    "\n",
    "# !huggingface-cli download city96/FLUX.1-dev-gguf flux1-dev-Q8_0.gguf --local-dir ./models/img\n",
    "\n",
    "# !huggingface-cli download black-forest-labs/FLUX.1-dev ae.safetensors --local-dir ./models/img\n",
    "# !huggingface-cli download comfyanonymous/flux_text_encoders clip_l.safetensors --local-dir ./models/img\n",
    "# !huggingface-cli download comfyanonymous/flux_text_encoders t5xxl_fp16.safetensors --local-dir ./models/img\n",
    "!huggingface-cli download kudzueye/boreal-flux-dev-v2 boreal-v2.safetensors --local-dir ./models/img/loras\n",
    "!huggingface-cli download kudzueye/Boreal boreal-flux-dev-lora-v04_1000_steps.safetensors --local-dir ./models/img/loras\n",
    "!mv ./models/img/loras/boreal-flux-dev-lora-v04_1000_steps.safetensors ./models/img/loras/boreal.safetensors\n",
    "!huggingface-cli download Shakker-Labs/FLUX.1-dev-LoRA-AntiBlur FLUX-dev-lora-AntiBlur.safetensors --local-dir ./models/img/loras\n",
    "!mv ./models/img/loras/FLUX-dev-lora-AntiBlur.safetensors ./models/img/loras/antiblur.safetensors\n",
    "\n",
    "# !wget https://github.com/Kitware/CMake/releases/download/v3.31.4/cmake-3.31.4-linux-x86_64.sh\n",
    "# !chmod +x cmake-3.31.4-linux-x86_64.sh\n",
    "# !sudo ./cmake-3.31.4-linux-x86_64.sh --prefix=/usr/local --skip-license \n",
    "# !rm cmake-3.31.4-linux-x86_64.sh\n",
    "# !cmake --version\n",
    "\n",
    "# !mkdir ~/stable-diffusion.cpp/build\n",
    "# %cd ~/stable-diffusion.cpp/build\n",
    "# !cmake .. -DSD_CUDA=ON\n",
    "# !cmake --build . --config Release\n",
    "\n",
    "# quantizing\n",
    "# !~/stable-diffusion.cpp/build/bin/sd --mode \"convert\" --type \"q8_0\" --model ~/models/img/temp/nr.safetensors\n",
    "\n",
    "import time\n",
    "start = time.time()\n",
    "\n",
    "# prompt = \"\"\"\n",
    "# mirror selfie photo of a smiling young redhead woman [<lora:boreal:0.3>|<lora:boreal-v2:0.15>|<lora:antilbur:2>]\n",
    "# \"\"\"\n",
    "prompt = \"\"\"\n",
    "selfie photo of a smiling young redhead woman<lora:boreal:1>\n",
    "\"\"\"\n",
    "\n",
    "# %cd ~/models/img\n",
    "# !~/stable-diffusion.cpp/build/bin/sd --diffusion-model flux1-dev-Q8_0.gguf --vae ae.safetensors --clip_l clip_l.safetensors \\\n",
    "# --t5xxl t5xxl_fp16.safetensors --steps 20 --cfg-scale 1.0 --sampling-method euler -v --diffusion-fa -W 1024 -H 1280 --seed 42 -p \"{prompt}\" --type q8_0 \\\n",
    "# --lora-model-dir ./loras --output ~/output.jpg --threads 16\n",
    "\n",
    "# print(f'took {time.time() - start} seconds')\n",
    "\n",
    "# %cd ~\n",
    "# from PIL import Image\n",
    "\n",
    "# with Image.open(\"output.png\") as img:\n",
    "#     img.convert(\"RGB\").save(\"output.jpg\", \"JPEG\")\n",
    "#     !rm output.png\n",
    "    \n",
    "# with Image.open(\"output.png\") as img:\n",
    "#     img.show()\n",
    "\n",
    "prompt = \"\"\"\n",
    "Night photo of a tall Mediterranean woman with long curly dark hair and a curvy body riding a bike at Times Square\n",
    "    \n",
    "[<lora:boreal:0.7>|<lora:boreal-v2:0.15>|<lora:antilbur:2>]\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# %cd ~/models/img\n",
    "# !~/stable-diffusion.cpp/build/bin/sd --mode img2img --diffusion-model flux1-dev-Q8_0.gguf --vae ae.safetensors --clip_l clip_l.safetensors \\\n",
    "# --t5xxl t5xxl_fp16.safetensors --steps 20 --cfg-scale 1.0 --sampling-method euler -v --diffusion-fa -W 1024 -H 1280 --seed 42 -p \"{prompt}\" --type q8_0 \\\n",
    "# --lora-model-dir ./loras \\\n",
    "# --output ~/output_img.jpg --threads 16 --init-img ~/output.jpg --strength 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4736607-938d-4144-89e5-4fddb5bd7e8b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from stable_diffusion_cpp import StableDiffusion\n",
    "import time\n",
    "\n",
    "base_path = \"./models/img\"\n",
    "start = time.time()\n",
    "flux = StableDiffusion(\n",
    "    diffusion_model_path=f\"{base_path}/flux1-dev-Q8_0.gguf\",\n",
    "    clip_l_path=f\"{base_path}/clip_l.safetensors\",\n",
    "    t5xxl_path=f\"{base_path}/t5xxl_fp16.safetensors\",\n",
    "    vae_path=f\"{base_path}/ae.safetensors\",\n",
    "    # vae_decode_only=True, # True for txt2img\n",
    "    diffusion_flash_attn=True,\n",
    "    lora_model_dir=f\"{base_path}/loras/\",\n",
    ")\n",
    "print(f'\\nloading took {time.time() - start}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9b2413d-7ee2-423e-9360-5528b4f8bb77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# txt2img\n",
    "%cd ~\n",
    "import time\n",
    "from PIL import Image\n",
    "\n",
    "start = time.time()\n",
    "output = flux.txt_to_img(\n",
    "    prompt=\"\"\"\n",
    "    \n",
    "    Photo of a young redhead woman with white clear skin,\n",
    "    beautiful Irish features,\n",
    "    field of tulips in the background, dusk, fireflies in the air\n",
    "    \n",
    "    [<lora:boreal:0.5>|<lora:antilbur:5>]\"\"\",\n",
    "    sample_steps=15,\n",
    "    width=768,\n",
    "    height=1024,\n",
    "    seed=1000,\n",
    "    cfg_scale=0.7,\n",
    "    guidance=5,\n",
    "    sample_method=\"euler\",\n",
    ")\n",
    "delta = round(time.time() - start)\n",
    "output[0].convert('RGB').save(\"output.jpg\", \"JPEG\")\n",
    "from IPython.display import clear_output\n",
    "clear_output()\n",
    "print(f'took {delta} seconds')\n",
    "with Image.open(\"output.jpg\") as img:\n",
    "    img.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38d53145-b6ea-45f6-9744-063c8986b2ea",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# img2img\n",
    "%cd ~\n",
    "import time\n",
    "from PIL import Image\n",
    "width, height = Image.open(\"output.jpg\").size\n",
    "start = time.time()\n",
    "output = flux.img_to_img(\n",
    "    prompt=\"\"\"\n",
    "    \n",
    "    Creepshot of a tall Mediterranean woman with long curly dark hair and a curvy body dining out in a fancy restaurant wearing small black dress.\n",
    "    \n",
    "    [<lora:boreal:0.7>|<lora:boreal-v2:0.15>|<lora:antilbur:2>]\"\"\",\n",
    "    negative_prompt=\"worst quality, smooth\",\n",
    "    image=\"output.jpg\", # The input image will be automatically resized to the match the width and height arguments (default: 512x512)\n",
    "    strength=1,\n",
    "    sample_steps=20,\n",
    "    width=width,\n",
    "    height=height,\n",
    "    seed=1000,\n",
    "    cfg_scale=1,\n",
    "    sample_method=\"euler\",\n",
    ")\n",
    "delta = round(time.time() - start)\n",
    "output[0].convert('RGB').save(\"output_img.jpg\")\n",
    "from IPython.display import clear_output\n",
    "clear_output()\n",
    "print(f'took {delta} seconds')\n",
    "output[0].show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
