{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc53df01",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# # one-time setup\n",
    "# # installing cmake\n",
    "# !wget https://github.com/Kitware/CMake/releases/download/v3.31.4/cmake-3.31.4-linux-x86_64.sh\n",
    "# !chmod +x cmake-3.31.4-linux-x86_64.sh\n",
    "# !sudo ./cmake-3.31.4-linux-x86_64.sh --prefix=/usr/local --skip-license \n",
    "# !rm cmake-3.31.4-linux-x86_64.sh\n",
    "# !cmake --version\n",
    "\n",
    "\n",
    "# # installing cuda\n",
    "# # !sudo apt remove --purge '^nvidia-.*' -y\n",
    "# !sudo apt autoremove -y\n",
    "# !wget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu$(lsb_release -rs | tr -d '.')/x86_64/cuda-keyring_1.0-1_all.deb\n",
    "# !sudo dpkg -i cuda-keyring_1.0-1_all.deb\n",
    "# # !rm cuda-keyring_1.0-1_all.deb\n",
    "# !sudo apt update\n",
    "# !sudo apt install cuda -y\n",
    "# !echo 'export PATH=/usr/local/cuda/bin:$PATH' >> ~/.bashrc\n",
    "# !echo 'export LD_LIBRARY_PATH=/usr/local/cuda/lib64:$LD_LIBRARY_PATH' >> ~/.bashrc\n",
    "# !source ~/.bashrc\n",
    "\n",
    "\n",
    "# # stable_diffusion_cpp_python\n",
    "# !sudo apt install build-essential g++-11 ninja-build\n",
    "# !sudo update-alternatives --install /usr/bin/gcc gcc /usr/bin/gcc-11 100\n",
    "# !sudo update-alternatives --install /usr/bin/g++ g++ /usr/bin/g++-11 100\n",
    "# !CMAKE_ARGS=\"-DCUDA_ARCH=sm_89 -DSD_CUDA=ON -DSD_FLASH_ATTN=ON\" pip install stable-diffusion-cpp-python --upgrade --force-reinstall --no-cache-dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10c60232-c71f-42a4-b876-bae76231a1b2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!export MAKEFLAGS=\"-j$(nproc)\"\n",
    "!pip install numpy torch hf_transfer huggingface-hub huggingface_hub[hf_transfer] flash-attn\n",
    "!pip install transformers accelerate optimum auto-gptq pickleshare safetensors pymongo json5 diffusers protobuf peft hf_transfer supabase \\\n",
    "    bitsandbytes torchvision pytorch_lightning xformers \"xfuser[diffusers,flash-attn]\" --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ed62d2c",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "!pip install python-dotenv supabase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81079794",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()  # This loads the variables from .env into os.environ\n",
    "\n",
    "import os\n",
    "from supabase import create_client, Client\n",
    "\n",
    "url: str = os.environ.get(\"SUPABASE_URL\")\n",
    "key: str = os.environ.get(\"SUPABASE_KEY\")\n",
    "supabase: Client = create_client(url, key)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fab83e6f",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "# Setting up HF transfer\n",
    "from huggingface_hub import login\n",
    "import base64\n",
    "t = 'aGZfaHZqck9VTXFvTXF3dW9HR3JoTlZKSWlsZUtFTlNQbXRjTw=='\n",
    "k = base64.b64decode(t.encode()).decode()\n",
    "login(token=k, add_to_git_credential=False)\n",
    "%env HUGGINGFACEHUB_API_TOKEN={k}\n",
    "%env HF_TOKEN={k}\n",
    "%env HF_HUB_ENABLE_HF_TRANSFER=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff745242",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# in terminal pip install hf_transfer\n",
    "# !huggingface-cli download black-forest-labs/FLUX.1-dev ae.safetensors\n",
    "!huggingface-cli download comfyanonymous/flux_text_encoders clip_l.safetensors\n",
    "!huggingface-cli download comfyanonymous/flux_text_encoders t5xxl_fp16.safetensors\n",
    "# !huggingface-cli download kudzueye/boreal-flux-dev-v2 boreal-v2.safetensors\n",
    "# !huggingface-cli download kudzueye/Boreal boreal-flux-dev-lora-v04_1000_steps.safetensors\n",
    "# !huggingface-cli download Shakker-Labs/FLUX.1-dev-LoRA-AntiBlur FLUX-dev-lora-AntiBlur.safetensors\n",
    "\n",
    "!huggingface-cli download black-forest-labs/FLUX.1-dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74036d58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ComfyUI setup\n",
    "%cd  /workspace/dtback\n",
    "!git clone https://github.com/comfyanonymous/ComfyUI.git\n",
    "%cd /workspace/dtback/ComfyUI\n",
    "!pip install -r requirements.txt\n",
    "%cd /workspace/dtback/ComfyUI/custom_nodes\n",
    "!git clone https://github.com/ltdrdata/ComfyUI-Manager comfyui-manager\n",
    "!git clone https://github.com/Suzie1/ComfyUI_Comfyroll_CustomNodes.git\n",
    "!git clone https://github.com/AlekPet/ComfyUI_Custom_Nodes_AlekPet.git # for ACE++ cn workflow\n",
    "\n",
    "!huggingface-cli download black-forest-labs/FLUX.1-dev flux1-dev.safetensors --local-dir /workspace/dtback/ComfyUI/models/unet\n",
    "!huggingface-cli download black-forest-labs/FLUX.1-Fill-dev flux1-fill-dev.safetensors --local-dir /workspace/dtback/ComfyUI/models/unet\n",
    "\n",
    "!huggingface-cli download black-forest-labs/FLUX.1-dev ae.safetensors --local-dir /workspace/dtback/ComfyUI/models/vae\n",
    "!huggingface-cli download comfyanonymous/flux_text_encoders clip_l.safetensors --local-dir /workspace/dtback/ComfyUI/models/clip\n",
    "!huggingface-cli download comfyanonymous/flux_text_encoders t5xxl_fp16.safetensors --local-dir /workspace/dtback/ComfyUI/models/clip\n",
    "\n",
    "%cd /workspace/dtback/ComfyUI/models/loras\n",
    "!huggingface-cli download kudzueye/Boreal boreal-flux-dev-lora-v04_1000_steps.safetensors --local-dir /workspace/dtback/ComfyUI/models/loras\n",
    "!huggingface-cli download Shakker-Labs/FLUX.1-dev-LoRA-AntiBlur FLUX-dev-lora-AntiBlur.safetensors --local-dir /workspace/dtback/ComfyUI/models/loras\n",
    "!huggingface-cli download Shakker-Labs/AWPortrait-FL AWPortrait-FL-lora.safetensors --local-dir /workspace/dtback/ComfyUI/models/loras\n",
    "!huggingface-cli download ali-vilab/ACE_Plus portrait/comfyui_portrait_lora64.safetensors --local-dir /workspace/dtback/ComfyUI/models/loras # ACE++\n",
    "!curl -L -J -O \"https://drive.google.com/uc?export=download&id=1UNuErS3bvfOUwGqGAPU0AA8hXKEOKKyD\"\n",
    "%cd /workspace/dtback/ComfyUI\n",
    "\n",
    "# sudo mkdir -p --mode=0755 /usr/share/keyrings\n",
    "# curl -fsSL https://pkg.cloudflare.com/cloudflare-main.gpg | sudo tee /usr/share/keyrings/cloudflare-main.gpg >/dev/null\n",
    "# echo 'deb [signed-by=/usr/share/keyrings/cloudflare-main.gpg] https://pkg.cloudflare.com/cloudflared any main' | sudo tee /etc/apt/sources.list.d/cloudflared.list\n",
    "# sudo apt-get update && sudo apt-get install cloudflared\n",
    "# cloudflared tunnel --url http://localhost:8000\n",
    "\n",
    "%cd /workspace/dtback\n",
    "!python3 ComfyUI/main.py --port 8000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b287fc63",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "cd /workspace/dtback\n",
    "git clone https://github.com/ali-vilab/ACE_plus.git\n",
    "cd ACE_plus\n",
    "# pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8406de52",
   "metadata": {},
   "outputs": [],
   "source": [
    "%env FLUX_FILL_PATH=hf://black-forest-labs/FLUX.1-Fill-dev\n",
    "%env PORTRAIT_MODEL_PATH=hf://ali-vilab/ACE_Plus@portrait/comfyui_portrait_lora64.safetensors        \n",
    "%env SUBJECT_MODEL_PATH=hf://ali-vilab/ACE_Plus@subject/comfyui_subject_lora16.safetensors        \n",
    "%env LOCAL_MODEL_PATH=hf://ali-vilab/ACE_Plus@local_editing/comfyui_local_lora16.safetensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f1ab7a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd /workspace/dtback/ACE_plus\n",
    "\n",
    "%env FLUX_FILL_PATH=hf://black-forest-labs/FLUX.1-Fill-dev\n",
    "%env ACE_PLUS_FFT_MODEL=hf://ali-vilab/ACE_Plus@ace_plus_fft.safetensors\n",
    "!python3 infer_fft.py\n",
    "%cd /workspace/dtback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dd8f66e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd /workspace/dtback/ACE_plus\n",
    "%env FLUX_FILL_PATH=hf://black-forest-labs/FLUX.1-Fill-dev\n",
    "%env ACE_PLUS_FFT_MODEL=hf://ali-vilab/ACE_Plus@ace_plus_fft.safetensors\n",
    "\n",
    "import argparse, importlib, io, os, sys\n",
    "\n",
    "from PIL import Image\n",
    "from scepter.modules.transform.io import pillow_convert\n",
    "from scepter.modules.utils.config import Config\n",
    "from scepter.modules.utils.file_system import FS\n",
    "\n",
    "if os.path.exists('__init__.py'):\n",
    "    package_name = 'scepter_ext'\n",
    "    spec = importlib.util.spec_from_file_location(package_name, '__init__.py')\n",
    "    package = importlib.util.module_from_spec(spec)\n",
    "    sys.modules[package_name] = package\n",
    "    spec.loader.exec_module(package)\n",
    "\n",
    "from inference.registry import INFERENCES\n",
    "fs_list = [\n",
    "    Config(cfg_dict={\"NAME\": \"HuggingfaceFs\", \"TEMP_DIR\": \"./cache\"}, load=False),\n",
    "    Config(cfg_dict={\"NAME\": \"ModelscopeFs\", \"TEMP_DIR\": \"./cache\"}, load=False),\n",
    "    Config(cfg_dict={\"NAME\": \"HttpFs\", \"TEMP_DIR\": \"./cache\"}, load=False),\n",
    "    Config(cfg_dict={\"NAME\": \"LocalFs\", \"TEMP_DIR\": \"./cache\"}, load=False),\n",
    "]\n",
    "\n",
    "for one_fs in fs_list:\n",
    "    FS.init_fs_client(one_fs)\n",
    "\n",
    "\n",
    "def run_one_case(pipe,\n",
    "                input_image = None,\n",
    "                input_mask = None,\n",
    "                input_reference_image = None,\n",
    "                save_path = \"examples/output/example.png\",\n",
    "                instruction = \"\",\n",
    "                output_h = 1024,\n",
    "                output_w = 1024,\n",
    "                seed = -1,\n",
    "                sample_steps = None,\n",
    "                guide_scale = None,\n",
    "                repainting_scale = None,\n",
    "                use_change=True,\n",
    "                keep_pixels=True,\n",
    "                keep_pixels_rate=0.8,\n",
    "                **kwargs):\n",
    "    if input_image is not None:\n",
    "        input_image = Image.open(io.BytesIO(FS.get_object(input_image)))\n",
    "        input_image = pillow_convert(input_image, \"RGB\")\n",
    "    if input_mask is not None:\n",
    "        input_mask = Image.open(io.BytesIO(FS.get_object(input_mask)))\n",
    "        input_mask = pillow_convert(input_mask, \"L\")\n",
    "    if input_reference_image is not None:\n",
    "        input_reference_image = Image.open(io.BytesIO(FS.get_object(input_reference_image)))\n",
    "        input_reference_image = pillow_convert(input_reference_image, \"RGB\")\n",
    "    print(repainting_scale)\n",
    "    image, _, _, _, seed = pipe(\n",
    "        reference_image=input_reference_image,\n",
    "        edit_image=input_image,\n",
    "        edit_mask=input_mask,\n",
    "        prompt=instruction,\n",
    "        output_height=output_h,\n",
    "        output_width=output_w,\n",
    "        sampler='flow_euler',\n",
    "        sample_steps=sample_steps or pipe.input.get(\"sample_steps\", 30),\n",
    "        guide_scale=guide_scale or pipe.input.get(\"guide_scale\", 50),\n",
    "        seed=seed,\n",
    "        repainting_scale=repainting_scale,\n",
    "        use_change=use_change,\n",
    "        keep_pixels=keep_pixels,\n",
    "        keep_pixels_rate=keep_pixels_rate\n",
    "    )\n",
    "    with FS.put_to(save_path) as local_path:\n",
    "        image.save(local_path)\n",
    "    return local_path, seed\n",
    "\n",
    "\n",
    "model_cfg = Config(load=True, cfg_file=\"config/ace_plus_fft.yaml\")\n",
    "pipe = INFERENCES.build(model_cfg)\n",
    "params = {\n",
    "    \"output_h\": 1024,\n",
    "    \"output_w\": 1024,\n",
    "    \"sample_steps\": 30,\n",
    "    \"guide_scale\": None,\n",
    "}\n",
    "def run():\n",
    "    module_name = \"examples.examples\"\n",
    "    if module_name in sys.modules:\n",
    "        del sys.modules[module_name]\n",
    "    mod = importlib.import_module(module_name)\n",
    "    all_examples = mod.fft_examples\n",
    "    importlib.reload(mod)\n",
    "    for example in all_examples:\n",
    "        example.update(params)\n",
    "        local_path, seed = run_one_case(pipe, **example)\n",
    "\n",
    "%cd /workspace/dtback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "978b5ec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd /workspace/dtback/ACE_plus\n",
    "run()\n",
    "%cd /workspace/dtback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0272bac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%env FLUX_FILL_PATH=hf://black-forest-labs/FLUX.1-Fill-dev\n",
    "%env PORTRAIT_MODEL_PATH=hf://ali-vilab/ACE_Plus@portrait/comfyui_portrait_lora64.safetensors        \n",
    "%env SUBJECT_MODEL_PATH=hf://ali-vilab/ACE_Plus@subject/comfyui_subject_lora16.safetensors        \n",
    "%env LOCAL_MODEL_PATH=hf://ali-vilab/ACE_Plus@local_editing/comfyui_local_lora16.safetensors\n",
    "%cd /workspace/dtback/ACE_plus\n",
    "!sed -i 's/SAMPLE_STEPS: 28/SAMPLE_STEPS: 30/' ./config/ace_plus_diffusers_infer.yaml\n",
    "!sed -i 's/assert cfg.args.task_type.upper() in task_model_cfg//g' infer_lora.py\n",
    "\n",
    "dir = \"/workspace/dtback/img\"\n",
    "!mkdir -p {dir}\n",
    "\n",
    "instruction = \"\"\"\n",
    "Keep face absolutely the same.\n",
    "Photo of a young woman riding a bike\n",
    "\"\"\".replace(\"'\", \"\")\n",
    "\n",
    "!python3 infer_lora.py --input_reference_image {dir}/reference.jpg --seed 42 --save_path {dir}/out.jpg --instruction '{instruction}' --output_h 1024 --output_w 1024 \\\n",
    "    --repainting_scale 0.7 --task_type portrait --cfg_folder ./config --infer_type diffusers --guide_scale 40\n",
    "\n",
    "%cd /workspace/dtback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63a9d4b5-62ac-40ba-82e7-1c57aaf789c5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "curl -s https://raw.githubusercontent.com/sivel/speedtest-cli/master/speedtest.py | python3 -\n",
    "export MAKEFLAGS=\"-j$(nproc)\"\n",
    "pip install numpy torch\n",
    "pip install flash-attn --no-build-isolation\n",
    "pip install transformers accelerate optimum auto-gptq huggingface-hub huggingface_hub[hf_transfer] pickleshare safetensors \\\n",
    "                pymongo json5 diffusers protobuf peft hf_transfer supabase bitsandbytes torchvision --upgrade\n",
    "pip install xformers \"xfuser[diffusers,flash-attn]\"\n",
    "pip install bitsandbytes torchvision\n",
    "pip install --upgrade transformers diffusers huggingface_hub peft\n",
    "pip install --force-reinstall -v \"triton==3.1.0\"\n",
    "\n",
    "k=$(echo \"aGZfaHZqck9VTXFvTXF3dW9HR3JoTlZKSWlsZUtFTlNQbXRjTw==\" | base64 --decode)\n",
    "# Log in to Hugging Face (this assumes Python and huggingface_hub are installed)\n",
    "python3 -c \"from huggingface_hub import login; login(token='$k', add_to_git_credential=False)\"\n",
    "export HUGGINGFACEHUB_API_TOKEN=\"$k\"\n",
    "export HF_TOKEN=\"$k\"\n",
    "export HF_HUB_ENABLE_HF_TRANSFER=1\n",
    "\n",
    "\n",
    "huggingface-cli download comfyanonymous/flux_text_encoders clip_l.safetensors\n",
    "huggingface-cli download comfyanonymous/flux_text_encoders t5xxl_fp16.safetensors\n",
    "huggingface-cli download black-forest-labs/FLUX.1-dev\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a8c9f83-d5f4-4861-ab17-899e2c67e0c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # generate the base\n",
    "!huggingface-cli download black-forest-labs/FLUX.1-dev\n",
    "\n",
    "%cd /workspace/dtback/models/img/loras/\n",
    "!curl -L -J -O \"https://drive.google.com/uc?export=download&id=1UNuErS3bvfOUwGqGAPU0AA8hXKEOKKyD\"\n",
    "%cd /workspace/dtback/\n",
    "\n",
    "import torch\n",
    "from diffusers import FluxPipeline\n",
    "\n",
    "if \"pipe\" not in globals():\n",
    "    pipe = FluxPipeline.from_pretrained(\"black-forest-labs/FLUX.1-dev\", device_map=\"balanced\", torch_dtype=torch.bfloat16)\n",
    "    pipe.load_lora_weights(\"kudzueye/Boreal\", weight_name=\"boreal-flux-dev-lora-v04_1000_steps.safetensors\", adapter_name=\"boreal\")\n",
    "    pipe.load_lora_weights(\"/workspace/dtback/models/img/loras\", weight_name=\"SameFace_Fix.safetensors\", adapter_name=\"sameface\")\n",
    "    pipe.set_adapters([\"boreal\", \"sameface\"], [0.7, -0.3])\n",
    "    pipe.fuse_lora()\n",
    "base_prompt = \"\"\"\"\n",
    "close photo of a fat indian woman in her 40's with huge bust and hips on solid plain gray background wearing corporate clothes.\n",
    "The woman is facing the camera directly\n",
    "\"\"\"\n",
    "base_prompt = \"\"\"\n",
    "Portrait shot photo of a 24-year-old Korean trainer from San Francisco on a solid plain gray background.\n",
    "She wears a yellow tank top that fits her comfortably, showing her toned arms and slight collarbones.\n",
    "Her skin is smooth and evenly toned with a natural dry glow.\n",
    "There is a soft light on her high cheekbones and a few tiny freckles across the bridge of her nose.\n",
    "Her face has a clear, oval shape with a defined yet gentle jawline.\n",
    "Fine lines on her forehead and around her eyes are very subtle, hinting at small expressions of life without any harsh marks.\n",
    "Her dark brown hair is straight and cut just to shoulder length, with a small side part that lets a few strands fall neatly across her face.\n",
    "The hair has a slight shine and frames her face evenly. Her dark brown eyes are almond-shaped and bright, with long lashes that add a natural definition.\n",
    "The eyes are set evenly on her face, and her neatly trimmed eyebrows follow a natural arch that gives a clear outline above her eyes.\n",
    "Her small, straight nose ends in a soft, rounded tip that fits well with the rest of her features.\n",
    "Her lips are naturally pink, kept closed in a slight smile, and her small chin is gently rounded.\n",
    "Her ears are small and sit close to her head, while her slender neck adds to the overall balanced look of her profile.\n",
    "\"\"\"\n",
    "img = pipe(prompt=base_prompt, guidance_scale=2, height=1024, width=1024, num_inference_steps=40,\n",
    "           generator=torch.Generator(\"cuda\").manual_seed(10)).images[0]\n",
    "img.show()\n",
    "img.save('prebase.webp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2877ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# flux plain\n",
    "import torch\n",
    "from diffusers import FluxPipeline\n",
    "\n",
    "if \"pipe\" not in globals():\n",
    "    pipe = FluxPipeline.from_pretrained(\"black-forest-labs/FLUX.1-dev\", device_map=\"balanced\", torch_dtype=torch.bfloat16)\n",
    "prompt = \"photo of a cat\"\n",
    "img = pipe(prompt=prompt, guidance_scale=2, height=1024, width=1024, num_inference_steps=40,\n",
    "           generator=torch.Generator(\"cuda\").manual_seed(10)).images[0]\n",
    "img.show()\n",
    "img.save('prebase.webp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d739688-c609-456c-b980-482d3385a2fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # remove bg from the base\n",
    "# https://github.com/PeterL1n/BackgroundMattingV2\n",
    "\n",
    "try:\n",
    "    del pipe\n",
    "except NameError:\n",
    "    pass\n",
    "import gc, torch\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "%cd /workspace/dtback/models/img/\n",
    "!pip install gdown  -q\n",
    "import gdown\n",
    "gdown.download('https://drive.google.com/uc?id=1-t9SO--H4WmP7wUl1tVNNeDkq47hjbv4', 'torchscript_resnet50_fp32.pth', quiet=False)\n",
    "%cd /workspace/dtback/\n",
    "\n",
    "import torch\n",
    "from torchvision.transforms.functional import to_tensor, to_pil_image\n",
    "from PIL import Image\n",
    "\n",
    "device = torch.device('cuda')\n",
    "precision = torch.float32\n",
    "\n",
    "model = torch.jit.load('/workspace/dtback/models/img/torchscript_resnet50_fp32.pth')\n",
    "model.backbone_scale = 0.2\n",
    "model.refine_mode = 'thresholding'\n",
    "model.refine_threshold = 0.0001\n",
    "model = model.to(device).eval()\n",
    "\n",
    "src = Image.open('prebase.webp').convert('RGB')\n",
    "bgr = Image.new(\"RGB\", src.size, (170, 170, 170)) \n",
    "\n",
    "src_tensor = to_tensor(src).unsqueeze(0).to(device, dtype=precision)\n",
    "bgr_tensor = to_tensor(bgr).unsqueeze(0).to(device, dtype=precision)\n",
    "\n",
    "pha, fgr = model(src_tensor, bgr_tensor)[:2]\n",
    "\n",
    "rgba = torch.cat([fgr, pha], dim=1)\n",
    "result_img = to_pil_image(rgba.squeeze(0).clamp(0, 1), mode='RGBA')\n",
    "result_img.show()\n",
    "result_img.save('base.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d76e06c6-5440-43aa-b8d7-2ad2d53bb93e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "cd /workspace/dtback\n",
    "git clone -b sd3 https://github.com/kohya-ss/sd-scripts.git\n",
    "cd sd-scripts\n",
    "pip install --upgrade -r requirements.txt\n",
    "mkdir -p loras\n",
    "rm -rf dataset\n",
    "mkdir -p dataset\n",
    "cd dataset\n",
    "# cp /workspace/dtback/base.png ./base.png\n",
    "# curl -O https://i.ibb.co/dwccwKCZ/watson.png # 1024\n",
    "# curl -O https://i.ibb.co/35PNsSrg/watson-512.png # 512\n",
    "cd ..\n",
    "\n",
    "cat > dataset.toml << EOF\n",
    "[[datasets]]\n",
    "resolution = 1024\n",
    "batch_size = 8\n",
    "keep_tokens = 1\n",
    "\n",
    "  [[datasets.subsets]]\n",
    "  image_dir = '/workspace/dtback/sd-scripts/dataset'\n",
    "  class_tokens = 'woman'\n",
    "  num_repeats = 5\n",
    "  flip_aug = true\n",
    "  face_crop_aug_range=[1.0, 1.4]\n",
    "EOF\n",
    "\n",
    "cd /workspace/dtback\n",
    "\n",
    "# create dataset of base on the different backgrounds\n",
    "python << 'EOF'\n",
    "import os\n",
    "from PIL import Image\n",
    "base_path = \"base.png\"\n",
    "output_folder = \"/workspace/dtback/sd-scripts/dataset\"\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "colors = [(255, 0, 0), (0, 255, 0), (0, 0, 255), (255, 255, 0), (255, 0, 255),\n",
    "          (0, 255, 255), (255, 128, 0), (128, 0, 128), (128, 128, 128), (255, 255, 255)]\n",
    "base = Image.open(base_path).convert(\"RGBA\")\n",
    "for i, (r, g, b) in enumerate(colors):\n",
    "    bg = Image.new(\"RGBA\", base.size, (r, g, b, 255))\n",
    "    out = Image.alpha_composite(bg, base)\n",
    "    out.save(os.path.join(output_folder, f\"char_bg_{i}.png\"))\n",
    "EOF\n",
    "\n",
    "\n",
    "cat > /root/.cache/huggingface/accelerate/default_config.yaml << EOF\n",
    "compute_environment: LOCAL_MACHINE\n",
    "debug: false\n",
    "distributed_type: 'NO'\n",
    "downcast_bf16: 'no'\n",
    "dynamo_config:\n",
    "  dynamo_backend: TENSORRT\n",
    "enable_cpu_affinity: true\n",
    "gpu_ids: all\n",
    "machine_rank: 0\n",
    "main_training_function: main\n",
    "mixed_precision: bf16\n",
    "num_machines: 1\n",
    "num_processes: 1\n",
    "rdzv_backend: static\n",
    "same_network: true\n",
    "tpu_env: []\n",
    "tpu_use_cluster: false\n",
    "tpu_use_sudo: false\n",
    "use_cpu: false\n",
    "EOF\n",
    "\n",
    "pip install bitsandbytes torchvision\n",
    "pip install --upgrade transformers diffusers huggingface_hub peft\n",
    "pip install --force-reinstall -v \"triton==3.1.0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a64a751-edf7-4b2b-84ef-11aadabac775",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "fpath=\"/root/.cache/huggingface/hub/models--black-forest-labs--FLUX.1-dev/snapshots/0ef5fff789c832c5c7f4e127f94c8b54bbcced44\"\n",
    "epath=\"/root/.cache/huggingface/hub/models--comfyanonymous--flux_text_encoders/snapshots/6af2a98e3f615bdfa612fbd85da93d1ed5f69ef5\"\n",
    "\n",
    "accelerate launch --mixed_precision bf16 --num_cpu_threads_per_process 10 sd-scripts/flux_train_network.py --pretrained_model_name_or_path \"${fpath}/flux1-dev.safetensors\" \\\n",
    "--clip_l \"${epath}/clip_l.safetensors\" --t5xxl \"${epath}/t5xxl_fp16.safetensors\" --ae \"${fpath}/ae.safetensors\" --cache_latents_to_disk --save_model_as safetensors --sdpa \\\n",
    "--persistent_data_loader_workers --max_data_loader_n_workers 10 --seed 0 --gradient_checkpointing --mixed_precision bf16 --save_precision bf16 --network_module networks.lora_flux \\\n",
    "--network_dim 8 --network_train_unet_only --optimizer_type adamw8bit --learning_rate 1e-4 --cache_text_encoder_outputs --highvram --max_train_epochs 4 --save_every_n_epochs 1 \\\n",
    "--dataset_config sd-scripts/dataset.toml --output_dir sd-scripts/loras --output_name wm --timestep_sampling shift --discrete_flow_shift 3.1582 --model_prediction_type raw \\\n",
    "--guidance_scale 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ac60914-5b53-4378-ad0b-0efcb948b681",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test the lora\n",
    "import torch\n",
    "from diffusers import FluxPipeline\n",
    "\n",
    "if \"pipe\" not in globals():\n",
    "    pipe = FluxPipeline.from_pretrained(\"black-forest-labs/FLUX.1-dev\", device_map=\"balanced\", torch_dtype=torch.bfloat16)\n",
    "    pipe.load_lora_weights(\"/workspace/dtback/sd-scripts/loras\", weight_name=\"wm.safetensors\", adapter_name=\"wm\")\n",
    "    pipe.load_lora_weights(\"kudzueye/Boreal\", weight_name=\"boreal-flux-dev-lora-v04_1000_steps.safetensors\", adapter_name=\"boreal\")\n",
    "    pipe.set_adapters([\"boreal\", \"wm\"], [0.0, 0.8])\n",
    "    pipe.fuse_lora()\n",
    "\n",
    "prompts = [\n",
    "    \"selfie photo of a woman at the Miami beach standing in water with wet hair\",\n",
    "    \"photo of a crying woman with a bouquet of flowers from her groom on her wedding\",\n",
    "    \"photo of a woman showering\",\n",
    "    \"photo of a happy woman playing with her dog\",\n",
    "    \"photo of a woman dining out in a fancy restraunt\",\n",
    "    \"photo of a woman relaxing on a sofa with a magasine\",\n",
    "    \"photo of a woman making funny face on her backyard\",\n",
    "]\n",
    "\n",
    "for i, prompt in enumerate(prompts):\n",
    "    img = pipe(\n",
    "        prompt=prompt, \n",
    "        guidance_scale=0, \n",
    "        height=1024, \n",
    "        width=1024, \n",
    "        num_inference_steps=20,\n",
    "        generator=torch.Generator(\"cuda\").manual_seed(1000)\n",
    "    ).images[0]\n",
    "\n",
    "    img.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae7bf9a1-3c6e-41ec-bdbf-d345425803b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "k=$(echo \"aGZfaHZqck9VTXFvTXF3dW9HR3JoTlZKSWlsZUtFTlNQbXRjTw==\" | base64 --decode)\n",
    "# Log in to Hugging Face (this assumes Python and huggingface_hub are installed)\n",
    "python3 -c \"from huggingface_hub import login; login(token='$k', add_to_git_credential=False)\"\n",
    "export HUGGINGFACEHUB_API_TOKEN=\"$k\"\n",
    "export HF_TOKEN=\"$k\"\n",
    "export HF_HUB_ENABLE_HF_TRANSFER=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b10514be-7595-4956-b094-436c83bcdaed",
   "metadata": {},
   "outputs": [],
   "source": [
    "!echo \"aGZfaHZqck9VTXFvTXF3dW9HR3JoTlZKSWlsZUtFTlNQbXRjTw==\" | base64 --decode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ef98401-a088-4911-a5d3-35b1522a2332",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%cd /workspace/dtback/\n",
    "!git clone -b dev https://github.com/camenduru/PuLID-FLUX-hf\n",
    "%cd PuLID-FLUX-hf\n",
    "!pip install apex gradio diffusers gradio fire insightface onnx onnxruntime-gpu onnxruntime accelerate timm SentencePiece git+https://github.com/XPixelGroup/BasicSR ftfy einops facexlib fire\n",
    "!wget https://huggingface.co/camenduru/PuLID/resolve/main/bpe_simple_vocab_16e6.txt.gz -O /workspace/dtback/PuLID-FLUX-hf/eva_clip/bpe_simple_vocab_16e6.txt.gz\n",
    "!python app.py\n",
    "%cd /workspace/dtback/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5717453c",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from diffusers import FluxPriorReduxPipeline, FluxPipeline\n",
    "from diffusers.utils import load_image\n",
    "\n",
    "if \"pipe\" not in globals():\n",
    "    pipe_prior_redux = FluxPriorReduxPipeline.from_pretrained(\"black-forest-labs/FLUX.1-Redux-dev\", torch_dtype=torch.bfloat16).to(\"cuda\")\n",
    "    pipe = FluxPipeline.from_pretrained(\"black-forest-labs/FLUX.1-dev\", torch_dtype=torch.bfloat16).to(\"cuda\")\n",
    "\n",
    "image = load_image(\"./base.webp\")\n",
    "\n",
    "pipe_prior_output = pipe_prior_redux(image)\n",
    "img_prompt_embeds = pipe_prior_output[\"prompt_embeds\"]\n",
    "\n",
    "text_prompt = \"woman sitting on an porch\"\n",
    "text_prompt_embeds, pooled_text_embeds, _ = pipe.encode_prompt(\n",
    "    prompt=text_prompt,\n",
    "    prompt_2=text_prompt,\n",
    "    device=\"cuda\",\n",
    "    num_images_per_prompt=1,\n",
    ")\n",
    "\n",
    "# Interpolate the text embeddings to match the image embeddings' sequence length.\n",
    "target_length = img_prompt_embeds.shape[1]  # e.g. 1241\n",
    "if text_prompt_embeds.shape[1] != target_length:\n",
    "    # Transpose to (B, embed_dim, seq_length)\n",
    "    text_prompt_embeds = text_prompt_embeds.transpose(1, 2)\n",
    "    # Interpolate along the sequence dimension.\n",
    "    text_prompt_embeds = F.interpolate(text_prompt_embeds, size=target_length, mode='linear', align_corners=False)\n",
    "    # Transpose back to (B, seq_length, embed_dim)\n",
    "    text_prompt_embeds = text_prompt_embeds.transpose(1, 2)\n",
    "\n",
    "# Blend the embeddings.\n",
    "alpha = 0.83\n",
    "blended_embeds = alpha * text_prompt_embeds + (1 - alpha) * img_prompt_embeds\n",
    "\n",
    "# For pooled embeddings, you might choose to use one source directly or blend if they are compatible.\n",
    "# Here, we simply use the pooled text embeddings.\n",
    "blended_pooled = pooled_text_embeds\n",
    "\n",
    "# Now call the Flux pipeline using the blended embeddings.\n",
    "img = pipe(\n",
    "    guidance_scale=0,\n",
    "    num_inference_steps=20,\n",
    "    generator=torch.Generator(\"cuda\").manual_seed(0),\n",
    "    prompt_embeds=blended_embeds,\n",
    "    pooled_prompt_embeds=blended_pooled,\n",
    "    **{k: v for k, v in pipe_prior_output.items() if k not in [\"prompt_embeds\", \"pooled_prompt_embeds\"]}\n",
    ").images[0]\n",
    "\n",
    "img.show()\n",
    "img.save(\"redux_blended.webp\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b6337fd",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# generate the rest\n",
    "import torch\n",
    "from diffusers import FluxImg2ImgPipeline\n",
    "from diffusers.utils import load_image\n",
    "\n",
    "if \"pipe\" not in globals():\n",
    "    pipe = FluxImg2ImgPipeline.from_pretrained(\"black-forest-labs/FLUX.1-dev\", torch_dtype=torch.bfloat16).to(\"cuda\")\n",
    "    pipe.load_lora_weights(\"kudzueye/Boreal\", weight_name=\"boreal-flux-dev-lora-v04_1000_steps.safetensors\", adapter_name=\"boreal\")\n",
    "    pipe.set_adapters(\"boreal\", 0.15)\n",
    "    pipe.fuse_lora()\n",
    "\n",
    "\n",
    "init_image = load_image(\"./1920_0.6.webp\")\n",
    "width, height = init_image.size\n",
    "prompt = \"photo of a woman in a snowy city\"\n",
    "\n",
    "img = pipe(\n",
    "    prompt=prompt, image=init_image, num_inference_steps=60, strength=0.6, guidance_scale=7.0,\n",
    "    generator=torch.Generator(\"cuda\").manual_seed(1000), width=width, height=height,\n",
    ").images[0]\n",
    "img.show()\n",
    "img.save('img.webp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "603d5b4b-4783-4b97-9fbc-479d84c58a4e",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# !rm -rf ~/.cache/huggingface\n",
    "# !rm -rf mongo_persistence && mkdir -p mongo_persistence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45981784-8d79-4c1b-84a7-f775d9592144",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Downloading models\n",
    "\n",
    "# !huggingface-cli download mlabonne/Meta-Llama-3.1-8B-Instruct-abliterated-GGUF meta-llama-3.1-8b-instruct-abliterated.Q8_0.gguf --local-dir ~/models/llms\n",
    "# !huggingface-cli download mlabonne/Meta-Llama-3.1-8B-Instruct-abliterated-GGUF meta-llama-3.1-8b-instruct-abliterated.Q4_K_M.gguf --local-dir ~/models/llms\n",
    "# !huggingface-cli download mlabonne/Meta-Llama-3.1-8B-Instruct-abliterated-GGUF meta-llama-3.1-8b-instruct-abliterated.Q2_K.gguf --local-dir ~/models/llms\n",
    "\n",
    "# !huggingface-cli download bartowski/Llama-3.3-70B-Instruct-abliterated-GGUF Llama-3.3-70B-Instruct-abliterated-IQ2_S.gguf --local-dir ~/models/llms\n",
    "\n",
    "# !huggingface-cli download mradermacher/Qwen2.5-32B-Instruct-abliterated-i1-GGUF Qwen2.5-32B-Instruct-abliterated.i1-Q4_K_S.gguf --local-dir ~/models/llms\n",
    "# !huggingface-cli download mradermacher/Qwen2.5-32B-Instruct-abliterated-i1-GGUF Qwen2.5-32B-Instruct-abliterated.i1-IQ3_M.gguf --local-dir ~/models/llms\n",
    "\n",
    "# !huggingface-cli download mradermacher/DeepSeek-R1-Distill-Qwen-32B-abliterated-i1-GGUF DeepSeek-R1-Distill-Qwen-32B-abliterated.i1-Q4_K_M.gguf --local-dir ~/models/llms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9796bb06-5295-4f66-8985-3c500d509c82",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Setting up LLama.cpp\n",
    "\n",
    "# !sudo apt install libcurl4-openssl-dev\n",
    "# !apt-cache policy nvidia-cuda-toolkit\n",
    "# !sudo apt remove nvidia-cuda-toolkit && sudo apt autoremove\n",
    "\n",
    "# !git clone https://github.com/ggerganov/llama.cpp --depth 1 --single-branch\n",
    "\n",
    "# %cd /teamspace/studios/this_studio/llama.cpp\n",
    "# !cmake -B build -DGGML_CUDA=ON -DGGML_CUDA_F16=ON -DGGML_CUDA_FA_ALL_QUANTS=ON\n",
    "# !cmake --build build --config Release -j 32\n",
    "# %cd ~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "710b8f60-22bf-404d-ac4d-1c4f80eb5f2c",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# testing llama.cpp tools\n",
    "\n",
    "# !.~/llama.cpp/build/bin/llama-cli\n",
    "%cd ~/llama.cpp/build/bin\n",
    "\n",
    "# !./llama-cli -m ~/llama.gguf -h\n",
    "# !./llama-cli -m ~/models/llama_6.gguf --gpu-layers 18 --ctx-size 131072 --batch-size 65536 -p \"I just \" --predict 1024 -no-cnv\n",
    "\n",
    "!./llama-server -m ~/models/llama_4.gguf --port 8000 --gpu-layers 33 --ctx-size 65536 --batch-size 65536\n",
    "\n",
    "# !./llama-bench -m ~/models/llama_4.gguf\n",
    "\n",
    "# !./llama-cli -m ~/models/llama_4.gguf --gpu-layers 33 --ctx-size 94208 --batch-size 65536 -p \"Sometimes \" --predict 128 -no-cnv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9969fd70-e374-481d-95a2-4823d8f82caa",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# short test generation\n",
    "print(requests.post(\"http://127.0.0.1:8000/v1/chat/completions\", json={\"messages\": [\n",
    "    {\n",
    "      \"role\": \"system\",\n",
    "      \"content\": \"You are a professional sexual romance writer.\",\n",
    "    },\n",
    "    {\n",
    "      \"role\": \"user\",\n",
    "      \"content\": \"make up a very explicit sexual scene\",          \n",
    "    }\n",
    "]}).json()['choices'][0]['message']['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6dd0576-21b2-4a93-9233-fa668ddeb01e",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "\"\"\" server start command\n",
    "~/llama.cpp/build/bin/llama-server -m ~/models/llms/llama_8.gguf --port 8000 --gpu-layers 33 --ctx-size 131072 \\\n",
    "--cache-type-k q8_0 --cache-type-v q8_0 --flash-attn\n",
    "\"\"\"\n",
    "\n",
    "\"\"\" start with grammar\n",
    "~/llama.cpp/build/bin/llama-server -m ~/models/llms/llama_4.gguf --port 8000 --gpu-layers 33 --ctx-size 131072 \\\n",
    "--cache-type-k q4_0 --cache-type-v q4_0 --flash-attn --grammar-file ~/generation/1_bots_text.gbnf\n",
    "~/llama.cpp/build/bin/llama-server -m ~/models/llms/llama_4.gguf --port 8000 --gpu-layers 33 --ctx-size 131072 \\\n",
    "--cache-type-k q4_0 --cache-type-v q4_0 --flash-attn --grammar-file ~/generation/2_add_img_prompts_to_bots.gbnf\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "949be372-3c4d-42d8-86a8-90dd6714095e",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# installing docker\n",
    "\n",
    "# !sudo apt install postgresql postgresql-contrib # installing postgres\n",
    "# !sudo systemctl enable postgresql\n",
    "# !sudo systemctl start postgresql\n",
    "\n",
    "# !sudo apt install -y apt-transport-https ca-certificates curl software-properties-common\n",
    "# !yes | curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo gpg --dearmor -o /usr/share/keyrings/docker-archive-keyring.gpg\n",
    "# !sudo add-apt-repository \\\"deb [arch=amd64] https://download.docker.com/linux/ubuntu focal stable\\\"\n",
    "# !apt-cache policy docker-ce\n",
    "# !sudo apt install docker-ce\n",
    "# !sudo systemctl status docker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "256762fa-ab1a-4b69-9370-5c7d239e4de3",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# start mongo\n",
    "!wget -qO- https://www.mongodb.org/static/pgp/server-8.0.asc | sudo tee /etc/apt/trusted.gpg.d/server-8.0.asc\n",
    "!echo \"deb [ arch=amd64,arm64 ] https://repo.mongodb.org/apt/ubuntu focal/mongodb-org/8.0 multiverse\" | sudo tee /etc/apt/sources.list.d/mongodb-org-8.0.list\n",
    "!sudo apt-get update\n",
    "!sudo apt-get install -y mongodb-mongosh\n",
    "\n",
    "!docker run --name mongodb -p 27017:27017 -v ~/mongo_persistence:/data/db -d mongo\n",
    "# mongosh --port 27017\n",
    "# docker stop mongodb && docker rm mongodb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b4702c0-4a91-4d98-936d-d16d9fa1e5cf",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## grammar builder https://grammar.intrinsiclabs.ai\n",
    "# !python3 generation/1_bots_text.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4859244-214b-464d-a9ef-4a28f4950659",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# stable-diffusion.cpp\n",
    "\n",
    "# %cd ~\n",
    "# !rm -rf stable-diffusion.cpp\n",
    "# !git clone --recursive https://github.com/leejet/stable-diffusion.cpp\n",
    "# %cd ~/stable-diffusion.cpp\n",
    "# !git pull origin master\n",
    "# !git submodule init\n",
    "# !git submodule update\n",
    "# %cd ~\n",
    "\n",
    "# !huggingface-cli download city96/FLUX.1-dev-gguf flux1-dev-Q8_0.gguf --local-dir ./models/img\n",
    "\n",
    "# !huggingface-cli download black-forest-labs/FLUX.1-dev ae.safetensors --local-dir ./models/img\n",
    "# !huggingface-cli download comfyanonymous/flux_text_encoders clip_l.safetensors --local-dir ./models/img\n",
    "# !huggingface-cli download comfyanonymous/flux_text_encoders t5xxl_fp16.safetensors --local-dir ./models/img\n",
    "!huggingface-cli download kudzueye/boreal-flux-dev-v2 boreal-v2.safetensors --local-dir ./models/img/loras\n",
    "!huggingface-cli download kudzueye/Boreal boreal-flux-dev-lora-v04_1000_steps.safetensors --local-dir ./models/img/loras\n",
    "!mv ./models/img/loras/boreal-flux-dev-lora-v04_1000_steps.safetensors ./models/img/loras/boreal.safetensors\n",
    "!huggingface-cli download Shakker-Labs/FLUX.1-dev-LoRA-AntiBlur FLUX-dev-lora-AntiBlur.safetensors --local-dir ./models/img/loras\n",
    "!mv ./models/img/loras/FLUX-dev-lora-AntiBlur.safetensors ./models/img/loras/antiblur.safetensors\n",
    "\n",
    "# !wget https://github.com/Kitware/CMake/releases/download/v3.31.4/cmake-3.31.4-linux-x86_64.sh\n",
    "# !chmod +x cmake-3.31.4-linux-x86_64.sh\n",
    "# !sudo ./cmake-3.31.4-linux-x86_64.sh --prefix=/usr/local --skip-license \n",
    "# !rm cmake-3.31.4-linux-x86_64.sh\n",
    "# !cmake --version\n",
    "\n",
    "# !mkdir -p ~/stable-diffusion.cpp/build\n",
    "# %cd ~/stable-diffusion.cpp/build\n",
    "# !cmake .. -DSD_CUDA=ON\n",
    "# !cmake --build . --config Release\n",
    "\n",
    "# quantizing\n",
    "# !~/stable-diffusion.cpp/build/bin/sd --mode \"convert\" --type \"q8_0\" --model ~/models/img/temp/nr.safetensors\n",
    "\n",
    "import time\n",
    "start = time.time()\n",
    "\n",
    "# prompt = \"\"\"\n",
    "# mirror selfie photo of a smiling young redhead woman [<lora:boreal:0.3>|<lora:boreal-v2:0.15>|<lora:antilbur:2>]\n",
    "# \"\"\"\n",
    "prompt = \"\"\"\n",
    "selfie photo of a smiling young redhead woman<lora:boreal:1>\n",
    "\"\"\"\n",
    "\n",
    "# %cd ~/models/img\n",
    "# !~/stable-diffusion.cpp/build/bin/sd --diffusion-model flux1-dev-Q8_0.gguf --vae ae.safetensors --clip_l clip_l.safetensors \\\n",
    "# --t5xxl t5xxl_fp16.safetensors --steps 20 --cfg-scale 1.0 --sampling-method euler -v --diffusion-fa -W 1024 -H 1280 --seed 42 -p \"{prompt}\" --type q8_0 \\\n",
    "# --lora-model-dir ./loras --output ~/output.jpg --threads 16\n",
    "\n",
    "# print(f'took {time.time() - start} seconds')\n",
    "\n",
    "# %cd ~\n",
    "# from PIL import Image\n",
    "\n",
    "# with Image.open(\"output.png\") as img:\n",
    "#     img.convert(\"RGB\").save(\"output.jpg\", \"JPEG\")\n",
    "#     !rm output.png\n",
    "    \n",
    "# with Image.open(\"output.png\") as img:\n",
    "#     img.show()\n",
    "\n",
    "prompt = \"\"\"\n",
    "Night photo of a tall Mediterranean woman with long curly dark hair and a curvy body riding a bike at Times Square\n",
    "    \n",
    "[<lora:boreal:0.7>|<lora:boreal-v2:0.15>|<lora:antilbur:2>]\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# %cd ~/models/img\n",
    "# !~/stable-diffusion.cpp/build/bin/sd --mode img2img --diffusion-model flux1-dev-Q8_0.gguf --vae ae.safetensors --clip_l clip_l.safetensors \\\n",
    "# --t5xxl t5xxl_fp16.safetensors --steps 20 --cfg-scale 1.0 --sampling-method euler -v --diffusion-fa -W 1024 -H 1280 --seed 42 -p \"{prompt}\" --type q8_0 \\\n",
    "# --lora-model-dir ./loras \\\n",
    "# --output ~/output_img.jpg --threads 16 --init-img ~/output.jpg --strength 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4736607-938d-4144-89e5-4fddb5bd7e8b",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from stable_diffusion_cpp import StableDiffusion\n",
    "import time\n",
    "\n",
    "base_path = \"./models/img\"\n",
    "start = time.time()\n",
    "flux = StableDiffusion(\n",
    "    diffusion_model_path=f\"{base_path}/flux1-dev-Q8_0.gguf\",\n",
    "    clip_l_path=f\"{base_path}/clip_l.safetensors\",\n",
    "    t5xxl_path=f\"{base_path}/t5xxl_fp16.safetensors\",\n",
    "    vae_path=f\"{base_path}/ae.safetensors\",\n",
    "    # vae_decode_only=True, # True for txt2img\n",
    "    diffusion_flash_attn=True,\n",
    "    lora_model_dir=f\"{base_path}/loras/\",\n",
    ")\n",
    "print(f'\\nloading took {time.time() - start}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9b2413d-7ee2-423e-9360-5528b4f8bb77",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# txt2img\n",
    "%cd ~\n",
    "import time\n",
    "from PIL import Image\n",
    "\n",
    "start = time.time()\n",
    "output = flux.txt_to_img(\n",
    "    prompt=\"\"\"\n",
    "    \n",
    "    Photo of a young redhead woman with white clear skin,\n",
    "    beautiful Irish features,\n",
    "    field of tulips in the background, dusk, fireflies in the air\n",
    "    \n",
    "    [<lora:boreal:0.5>|<lora:antilbur:5>]\"\"\",\n",
    "    sample_steps=15,\n",
    "    width=768,\n",
    "    height=1024,\n",
    "    seed=1000,\n",
    "    cfg_scale=0.7,\n",
    "    guidance=5,\n",
    "    sample_method=\"euler\",\n",
    ")\n",
    "delta = round(time.time() - start)\n",
    "output[0].convert('RGB').save(\"output.jpg\", \"JPEG\")\n",
    "from IPython.display import clear_output\n",
    "clear_output()\n",
    "print(f'took {delta} seconds')\n",
    "with Image.open(\"output.jpg\") as img:\n",
    "    img.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38d53145-b6ea-45f6-9744-063c8986b2ea",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# img2img\n",
    "%cd ~\n",
    "import time\n",
    "from PIL import Image\n",
    "width, height = Image.open(\"output.jpg\").size\n",
    "start = time.time()\n",
    "output = flux.img_to_img(\n",
    "    prompt=\"\"\"\n",
    "    \n",
    "    Creepshot of a tall Mediterranean woman with long curly dark hair and a curvy body dining out in a fancy restaurant wearing small black dress.\n",
    "    \n",
    "    [<lora:boreal:0.7>|<lora:boreal-v2:0.15>|<lora:antilbur:2>]\"\"\",\n",
    "    negative_prompt=\"worst quality, smooth\",\n",
    "    image=\"output.jpg\", # The input image will be automatically resized to the match the width and height arguments (default: 512x512)\n",
    "    strength=1,\n",
    "    sample_steps=20,\n",
    "    width=width,\n",
    "    height=height,\n",
    "    seed=1000,\n",
    "    cfg_scale=1,\n",
    "    sample_method=\"euler\",\n",
    ")\n",
    "delta = round(time.time() - start)\n",
    "output[0].convert('RGB').save(\"output_img.jpg\")\n",
    "from IPython.display import clear_output\n",
    "clear_output()\n",
    "print(f'took {delta} seconds')\n",
    "output[0].show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
